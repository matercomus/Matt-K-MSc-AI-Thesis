\section{Introduction}
\label{sec:introduction}

Accurate trajectory prediction is foundational to intelligent transportation systems, underpinning applications from dynamic navigation and fleet dispatch to digital-twin simulation of urban flow.  Modern cities contain tens of thousands of interconnected road segments; a practical predictor must therefore reason over large graphs
while delivering sub-second latency at metropolitan scale.

State-of-the-art transformer models excel at learning long-range spatial dependencies, yet their quadratic self-attention incurs inference times incompatible with real-time traffic management.  Conversely, lightweight graph-aware models such as\ HOSER achieve millisecond-level speed but fall short in route-completion accuracy.  This accuracyâ€“latency dichotomy poses a central research challenge: \emph{how can one inherit the rich spatial knowledge of heavy models without deploying them at run time?}

This thesis answers the question by distilling the transformer-based LM-TAD anomaly detector into the hierarchical, low-latency HOSER predictor \emph{during training only}.  Our cross-task distillation transfers spatial priors learned in anomaly detection to next-step prediction, yielding a student that approaches transformer accuracy while preserving operational efficiency.

\paragraph{Contributions.}  We make four key contributions:
\begin{itemize}
  \item Propose the first cross-task distillation framework that transfers spatial knowledge from trajectory anomaly detection to trajectory prediction.
  \item Develop a batched, GPU-optimised KL divergence module that enables large-scale training with negligible overhead.
  \item Empirically validate the distilled model on large-scale urban datasets, showing pronounced improvements across established mobility-generation metrics while maintaining millisecond-level inference latency.
  \item Release an end-to-end evaluation pipeline for trajectory generation and similarity analysis, facilitating reproducible research.
\end{itemize}

\paragraph{Paper organisation.}  \autoref{sec:lit-review} surveys the evolution of trajectory modelling, culminating in the need for knowledge distillation.  \autoref{sec:methodology} details the LM-TAD\,$\rightarrow$\,HOSER distillation algorithm and training pipeline.  \autoref{sec:data-preprocessing} describes dataset preparation, and \autoref{sec:evaluation} presents empirical results.  We conclude with future research directions in \autoref{sec:conclusion}.

