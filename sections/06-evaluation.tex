% Source: notes/hoser/hoser-distill-optuna-6/EVALUATION_ANALYSIS.md
% Source: notes/hoser/hoser-distill-optuna-6/RESULTS_ANALYSIS.md
% Figures: assets/plots/hoser/

\section{Experimental Evaluation}
\label{sec:evaluation}

This section presents a comprehensive empirical evaluation of knowledge distillation for trajectory prediction. We begin by describing the experimental setup and evaluation metrics (\autoref{sec:eval-setup}, \autoref{sec:eval-metrics}), then present detailed results for the Beijing dataset (\autoref{sec:eval-beijing}) and Porto dataset (\autoref{sec:eval-porto}). We conclude with cross-dataset analysis and inference speed discussion (\autoref{sec:eval-cross}, \autoref{sec:eval-inference}).

\subsection{Experimental Setup}
\label{sec:eval-setup}

\subsubsection{Models Evaluated}

We compare two model configurations trained under identical conditions, differing only in whether distillation is enabled:

\textbf{Vanilla HOSER (Baseline):} Standard maximum likelihood training without teacher guidance. This corresponds to Trial 0 in our hyperparameter optimization, with distillation weight $\lambda = 0$. The vanilla model learns exclusively from hard labels (ground-truth next roads).

\textbf{Distilled HOSER (Proposed):} HOSER student trained with knowledge distillation from the frozen LM-TAD teacher. Optimal hyperparameters (distillation weight $\lambda$, temperature $\tau$, and teacher window size $w$) are dataset-specific and identified via systematic Optuna search (details in \autoref{sec:impl-hparam}; optimal values in Table~\ref{tab:distill-hparams}). The distilled model learns from both hard labels and soft teacher distributions.

\subsubsection{Fair Comparison Protocol}

To isolate the effect of knowledge distillation, all other training parameters remain identical between vanilla and distilled models (\hyperref[app:training-config]{Table~\ref*{tab:training-config-appendix}, Appendix~\ref*{app:training-config}}). Both use HOSER architecture with AdamW optimizer ($\eta = 5 \times 10^{-4}$), 25 epochs with cosine annealing, and identical train/val/test splits. The \emph{only difference} is distillation: vanilla sets $\lambda = 0$ (disabled), while distilled uses optimal dataset-specific hyperparameters (Table~\ref{tab:distill-hparams}).

This controlled experimental design ensures that performance differences stem purely from knowledge transfer, not confounding factors like different learning rates or architectures. Note: batch sizes vary by dataset due to memory constraints with longer trajectories (see \autoref{sec:impl-practical}).

\subsubsection{Hyperparameter Tuning}

Optimal distillation hyperparameters are dataset-specific and were identified via systematic Optuna search with the CMA-ES sampler (\autoref{sec:impl-hparam}). Each dataset requires independent hyperparameter optimization (see Table~\ref{tab:distill-hparams} for optimal values per dataset). Trial 0 establishes the vanilla baseline ($\lambda = 0$) for each dataset, validated with multiple random seeds.

\subsubsection{Trajectory Generation Protocol}

For evaluation, we generate synthetic trajectories using beam search with width $b = 4$:

\begin{enumerate}[noitemsep,topsep=0pt]
    \item Sample 5,000 origin-destination (OD) pairs from real training set (memorization test)
    \item Sample 5,000 OD pairs from real test set (generalization test)
    \item For each OD pair, generate a complete trajectory using the trained model
    \item Compare generated trajectories against real trajectories with matching OD pairs
\end{enumerate}

This protocol separately assesses \emph{memorization} (train OD) and \emph{generalization} (test OD), revealing whether models merely overfit training patterns or learn transferable spatial reasoning.

\subsection{Evaluation Metrics}
\label{sec:eval-metrics}

We employ a comprehensive metric suite covering global distribution quality, local trajectory similarity, and path completion capability. Most metrics follow the HOSER evaluation framework~\cite{caoHolisticSemanticRepresentation2025,HOSEREvaluationMainipynb} to enable direct comparison. Our key methodological contribution is the systematic generation and evaluation on both training OD pairs and held-out test OD pairs separately---extending HOSER's approach of sampling OD pairs from the training distribution only. This train/test comparative analysis enables assessment of memorization versus generalization, revealing that distilled models learn transferable spatial reasoning rather than overfitting training routes. We also introduce OD pair matching rate as an explicit metric for path completion success. Detailed formulations are provided in \hyperref[app:metrics]{Appendix~\ref*{app:metrics}}.

\subsubsection{Global Distribution Metrics}

These metrics assess whether aggregate statistics of generated trajectories match real data distributions:

\begin{itemize}[noitemsep,topsep=0pt]
    \item \textbf{Jensen-Shannon Divergence (JSD)}: Symmetric divergence measure (bounded $[0, 1]$) computed for distance, duration, and radius of gyration distributions. Lower values indicate better distribution matching.
    \item \textbf{Radius of Gyration}: Measures spatial spread of trajectories, capturing how geographically dispersed a trajectory is.
\end{itemize}

\subsubsection{Local Trajectory Metrics}

These metrics compare individual trajectory pairs with matching OD endpoints:

\begin{itemize}[noitemsep,topsep=0pt]
    \item \textbf{Hausdorff Distance}: Maximum spatial deviation between trajectories. Captures worst-case error but scales with trajectory length.
    \item \textbf{Dynamic Time Warping (DTW)}: Cumulative distance under optimal temporal alignment. Handles different sampling rates but also scales with length.
    \item \textbf{Edit Distance on Real sequence (EDR)}: Normalized edit operations (100m threshold). Length-normalized ($\in [0,1]$) and robust to outliers.
\end{itemize}

\subsubsection{Coverage Metrics}

\textbf{OD Pair Matching Rate}: Percentage of generated trajectories whose \emph{actual endpoints} match real OD pairs.

\textbf{Critical distinction}: The model receives target OD $(r_o, r_d)$ but may fail to reach $r_d$. We extract the \emph{generated trajectory's actual endpoints} and check if this OD pair exists in real data via grid-based spatial binning (0.001Â°, $\sim$111m). High matching rates indicate path completion success and realistic mobility patterns. Low rates reveal fundamental navigation failures.

\subsubsection{Scenario-Level Evaluation}

To assess context-dependent performance, we partition trajectories into 9 spatial and temporal scenarios using predefined dataset-specific scenario sets: temporal contexts (weekday, weekend, peak, off-peak) and spatial contexts (city center, suburban, within center, to center, from center). For each scenario, we compute aggregate metrics (OD match rate, Distance/Radius JSD) and report per-scenario performance deltas ($\Delta$) between distilled and vanilla models. This granular analysis reveals where distillation provides benefit versus where vanilla suffices, avoiding the averaging effect of aggregate metrics that can mask context-dependent performance.

\subsection{Results: Beijing Dataset}
\label{sec:eval-beijing}

We present comprehensive results for the Beijing HOSER reference dataset, comparing vanilla and distilled models across all metrics.

\subsubsection{Catastrophic Vanilla Failure vs Distilled Success}

The most critical finding is the dramatic difference in path completion capability demonstrated by the dramatic OD matching rates. Distilled models achieve \textbf{85.7--88.2\% OD matching rates}, indicating successful navigation to target destinations. In stark contrast, the vanilla baseline achieves only \textbf{12.1\% on test data}, failing to complete 88\% of trajectories. This represents a \textbf{+73-76 percentage point improvement} from distillation.

\paragraph{Distribution Quality Improvements}
Knowledge distillation produces dramatic improvements in trajectory realism:
\begin{itemize}[leftmargin=*,noitemsep]
    \item \textbf{87--89\% reduction in Distance JSD}: Vanilla 0.145--0.153 $\to$ Distilled 0.016--0.022
    \item \textbf{98\% reduction in Radius JSD}: Vanilla 0.198--0.206 $\to$ Distilled 0.003--0.004
    \item \textbf{Realistic trip lengths}: Distilled generates 6.3--6.7 km trips (vs real 5.16 km baseline), while vanilla produces unrealistically short 2.3--2.4 km trips (55\% below real)
\end{itemize}

\paragraph{Scenario-Level Analysis}
Distillation benefits are universal across spatial and temporal contexts. All 9 evaluated scenarios (city\_center, suburban, weekday, weekend, peak, off\_peak, within\_center, to\_center, from\_center) show large improvements, with long-distance navigation benefiting most: \texttt{from\_center} scenarios improve by $\Delta$ Distance JSD = $-0.242$, \texttt{to\_center} by $-0.196$.

\subsubsection{Path Completion Success}

Figure~\ref{fig:od-matching} shows the OD pair matching rates, our most critical metric revealing whether models can successfully navigate to destinations.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{assets/plots/hoser/od_matching_rates.pdf}
    \caption{OD pair matching rates for vanilla vs. distilled models on train and test OD pairs. Distilled models achieve 85--89\% success, while vanilla fails 82--88\% of the time.}
    \label{fig:od-matching}
\end{figure}

Table~\ref{tab:od-results} quantifies the dramatic difference in path completion capability.

\begin{table}[h]
    \centering
    \caption{Path completion success on Beijing dataset}
    \label{tab:od-results}
    \small
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Model}                           & \textbf{Seed}           & \textbf{Train OD}       & \textbf{Test OD} & \textbf{Train Match} & \textbf{Test Match} \\
        \midrule
        Distilled                                & 42                      & 4,254 / 4,960           & 4,204 / 4,907    & 85.8\%               & 85.7\%              \\
        Distilled                                & 44                      & 4,433 / 4,959           & 4,333 / 4,910    & 89.4\%               & 88.2\%              \\
        Vanilla                                  & 42                      & 824 / 4,654             & 557 / 4,610      & 17.7\%               & 12.1\%              \\
        \midrule
        \multicolumn{4}{l}{\textbf{Improvement}} & \textbf{47--74$\times$} & \textbf{60--73$\times$}                                                                 \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Key findings:}
\begin{itemize}[noitemsep,topsep=0pt]
    \item Distilled models successfully reach target destinations 85--89\% of the time
    \item Vanilla models fail to complete paths 82--88\% of the time, indicating fundamental spatial reasoning deficits
    \item Performance is consistent across train and test OD pairs, demonstrating true generalization rather than memorization
    \item Seed robustness is high (85.8\% vs 89.4\%), confirming reliable knowledge transfer
\end{itemize}

\subsubsection{Distribution Quality}

Figure~\ref{fig:distance-distributions} compares trip distance distributions between real data and generated trajectories.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{assets/plots/hoser/distance_distributions.pdf}
    \caption{Trip distance distributions for real vs. generated trajectories. Distilled models match real distributions closely (JSD = 0.016--0.022), while vanilla generates unrealistically short trips (JSD = 0.145--0.153).}
    \label{fig:distance-distributions}
\end{figure}

Table~\ref{tab:jsd-results} quantifies distribution matching quality via JSD metrics.

\begin{table}[h]
    \centering
    \caption{Distribution quality (JSD) on Beijing dataset - lower is better}
    \label{tab:jsd-results}
    \small
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Model}                                        & \textbf{Seed}                         & \textbf{Distance JSD} & \textbf{Radius JSD} & \textbf{Avg. Distance (km)} \\
        \midrule
        Real (train)                                          & --                                    & --                    & --                  & 5.16                        \\
        Real (test)                                           & --                                    & --                    & --                  & 5.16                        \\
        \midrule
        Distilled                                             & 42                                    & 0.0192--0.0217        & 0.0034--0.0038      & 6.48--6.68                  \\
        Distilled                                             & 44                                    & 0.0162--0.0178        & 0.0028--0.0034      & 6.34--6.44                  \\
        Vanilla                                               & 42                                    & 0.1445--0.1528        & 0.1979--0.2057      & 2.33--2.43                  \\
        \midrule
        \multicolumn{3}{l}{\textbf{Distance JSD improvement}} & \multicolumn{2}{c}{\textbf{87--89\%}}                                                                             \\
        \multicolumn{3}{l}{\textbf{Radius JSD improvement}}   & \multicolumn{2}{c}{\textbf{98\%}}                                                                                 \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Key findings:}
\begin{itemize}[noitemsep,topsep=0pt]
    \item Distilled models achieve near-perfect distance distribution matching (JSD $<$ 0.022)
    \item Vanilla models generate trajectories that are 55\% shorter than reality (2.4 km vs 5.2 km)
    \item Radius of gyration matching improves by 98\%, indicating distilled models capture spatial complexity
    \item Distilled models slightly overestimate trip length (6.4 km vs 5.2 km), a conservative bias
\end{itemize}

Figure~\ref{fig:jsd-comparison} provides a comprehensive view of all distribution metrics.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{assets/plots/hoser/jsd_comparison.pdf}
    \caption{Comprehensive JSD comparison across distance, duration, and radius of gyration. Distilled models (blue) dramatically outperform vanilla (red) on all metrics.}
    \label{fig:jsd-comparison}
\end{figure}

\subsubsection{Generalization vs. Memorization}

A critical question: do distilled models merely memorize training patterns or learn generalizable spatial reasoning?

Figure~\ref{fig:train-test} compares performance on training OD pairs (seen during training) versus test OD pairs (unseen).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/plots/hoser/train_test_comparison.pdf}
    \caption{Train vs. test performance comparison. Distilled models perform \emph{better} on test than train (lower JSD), indicating true spatial generalization. Vanilla degrades on test.}
    \label{fig:train-test}
\end{figure}

\textbf{Key findings:}
\begin{itemize}[noitemsep,topsep=0pt]
    \item Distilled models: Test JSD \emph{lower} than train JSD (0.0162 vs 0.0178 for seed 44)
    \item This counter-intuitive result indicates the model learned generalizable spatial patterns, not route memorization
    \item Vanilla models: Test JSD higher than train JSD (0.1528 vs 0.1445), showing typical overfitting
    \item Consistent trip lengths across train/test for distilled (6.34--6.68 km), confirming stable spatial understanding
\end{itemize}

\subsubsection{Seed Robustness}

To assess whether distillation reliably transfers knowledge, we train with multiple random seeds.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{assets/plots/hoser/seed_robustness.pdf}
    \caption{Cross-seed consistency for distilled models. Coefficient of variation (CV) below 15\% across all metrics indicates reliable knowledge transfer.}
    \label{fig:seed-robustness}
\end{figure}

\textbf{Key findings:}
\begin{itemize}[noitemsep,topsep=0pt]
    \item Distance JSD: CV = 8.9\% (very stable)
    \item Radius JSD: CV = 14.1\% (stable)
    \item OD coverage: CV = 2.2\% (extremely stable)
    \item Minimal variation confirms distillation is robust to initialization
\end{itemize}

\subsubsection{Local Trajectory Metrics}

Figure~\ref{fig:local-metrics} presents trajectory-level similarity measures.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/plots/hoser/local_metrics.pdf}
    \caption{Local trajectory metrics. Note: Lower values for vanilla reflect shorter trajectories, not better quality.}
    \label{fig:local-metrics}
\end{figure}

\begin{table}[h]
    \centering
    \caption{Local trajectory metrics on Beijing dataset}
    \label{tab:local-results}
    \small
    \begin{tabular}{lccc}
        \toprule
        \textbf{Model}      & \textbf{Hausdorff (km)} & \textbf{DTW (km)} & \textbf{EDR} \\
        \midrule
        Distilled (seed 42) & 0.95--1.00              & 27.6--29.0        & 0.488--0.505 \\
        Distilled (seed 44) & 0.95--0.97              & 27.6--28.4        & 0.483--0.506 \\
        Vanilla             & 0.51--0.56              & 7.7--8.6          & 0.504--0.513 \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Important interpretation:} Vanilla's lower Hausdorff and DTW values are \emph{not} indicators of better quality. These metrics scale with trajectory lengthâvanilla's shorter trips (2.4 km vs 6.4 km) naturally have smaller cumulative distances. When normalized by trip length:

\begin{itemize}[noitemsep,topsep=0pt]
    \item Distilled DTW per km: 28 / 6.4 = 4.4 km/km
    \item Vanilla DTW per km: 8 / 2.4 = 3.3 km/km
\end{itemize}

Even accounting for length, distilled models remain competitive while generating \emph{realistic-length} trajectoriesâthe critical requirement.

EDR (normalized metric) shows similar values across models ($\sim$0.50), indicating comparable alignment quality when trajectory length is factored out.

\subsubsection{Comprehensive Performance Summary}

Figure~\ref{fig:performance-radar} synthesizes all metrics into a radar chart.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{assets/plots/hoser/performance_radar.pdf}
    \caption{Normalized performance radar chart. Distilled models (blue) dominate across all dimensions. Scores computed as: OD coverage (raw \%), Distance quality (1 - JSD), Radius quality (1 - JSD), Distance accuracy (1 - |real - gen| / real).}
    \label{fig:performance-radar}
\end{figure}

\subsection{Results: Porto Dataset}
\label{sec:eval-porto}

We evaluate on the Porto taxi dataset using Phase 1 optimal hyperparameters ($\lambda=0.00644$, $\tau=2.802$, $w=4$) identified through a 20-trial Optuna study. Three seeds were evaluated for both vanilla and distilled models (seeds 42, 43, 44), generating 5,000 trajectories per model.

\subsubsection{Contrasting Behavior: Both Models Succeed}

Unlike Beijing's catastrophic vanilla failure, Porto reveals fundamentally different dynamics where both vanilla and distilled models achieve high path completion success, with marginal or context-dependent distillation benefits.

\subsubsection{Path Completion Success}

Porto demonstrates that vanilla HOSER can successfully navigate when trajectories are sufficiently short:
\begin{itemize}[leftmargin=*,noitemsep]
    \item \textbf{Vanilla}: 88.8\% $\pm$ 0.9\% OD match rate (test set, mean $\pm$ std across seeds)
    \item \textbf{Distilled}: 86.9\% $\pm$ 2.9\% OD match rate (test set)
    \item \textbf{Marginal difference}: $-$1.9 percentage points (distilled slightly lower)
\end{itemize}

\textbf{Trajectory length hypothesis:} Porto trajectories average 3.66 km (29\% shorter than Beijing's 5.16 km), making the navigation task more tractable for vanilla MLE-only training. Shorter trajectories require fewer sequential decisions, reducing cumulative error and allowing vanilla models to successfully reach targets without teacher guidance.

\subsubsection{Distribution Quality}

Aggregate distribution metrics show minimal distillation benefit on Porto:
\begin{itemize}[leftmargin=*,noitemsep]
    \item \textbf{Distance JSD}: Vanilla 0.0055 vs Distilled 0.0060 (+9\%, vanilla better)
    \item \textbf{Radius JSD}: Vanilla 0.0106 vs Distilled 0.0108 (+2\%, vanilla better)
\end{itemize}

The minimal differences (both models achieve JSD $<$ 0.011) indicate that when vanilla succeeds at path completion, distributional guidance from distillation provides limited additional benefit.

\subsubsection{Scenario-Level Analysis}

While aggregate metrics show marginal benefit, scenario-level analysis reveals context-dependent performance:
\begin{itemize}[leftmargin=*,noitemsep]
    \item \textbf{Dense urban contexts}: Distilled excels (within center: $\Delta$ Distance JSD = $-0.0104$, distilled better)
    \item \textbf{Suburban contexts}: Vanilla excels (suburban: $\Delta$ Distance JSD = $+0.0074$, vanilla better)
\end{itemize}

This spatial heterogeneity suggests teacher (LM-TAD) knowledge is localized to training context (city center), providing benefit in dense urban navigation but not in suburban areas.

\subsubsection{Dataset-Specific Hyperparameters}

Porto requires 4.3$\times$ higher $\lambda$ (0.00644 vs Beijing's 0.0014), 42\% lower $\tau$ (2.802 vs 4.37), and 43\% shorter window (4 vs 7). Memory constraints necessitated batch size reduction (32 vs Beijing's 128), creating an uncontrolled confound that prevents isolating whether hyperparameter divergence stems from dataset characteristics or batch size effects.

\subsection{Cross-Dataset Analysis}
\label{sec:eval-cross}

\textcolor{red}{[TO BE COMPLETED AFTER PORTO PHASE 2 EVALUATION]}

\textbf{Planned analyses:}
\begin{itemize}[noitemsep,topsep=0pt]
    \item Compare distillation effectiveness (JSD improvements, OD matching gains) across Beijing and Porto datasets
    \item Identify dataset characteristics that influence knowledge transfer quality
    \item Assess whether optimal hyperparameters ($\lambda$, $\tau$, $w$) generalize across cities
    \item Analyze relationship between network size, trajectory length, and distillation benefits
    \item Evaluate cross-dataset generalization: train on Beijing, test on Porto
\end{itemize}

\subsection{Inference Speed Analysis}
\label{sec:eval-inference}

A core motivation for knowledge distillation is achieving transformer-level accuracy while preserving the student's fast inference speed. We analyze the computational characteristics of our distillation framework.

\subsubsection{Training-Time Overhead}

The distillation framework introduces teacher forward passes during training but requires no teacher at inference time. Measured training overhead on Beijing dataset (NVIDIA RTX 3090, batch size 128):

\begin{itemize}[noitemsep,topsep=0pt]
    \item \textbf{Vanilla HOSER}: $\sim$15 minutes/epoch (629k trajectories)
    \item \textbf{Distilled HOSER}: $\sim$16.5 minutes/epoch (+10\% overhead)
    \item \textbf{Teacher overhead}: $<$10\% of total training time (frozen weights, efficient batching)
\end{itemize}

The modest training overhead ($\sim$1.5 minutes/epoch) is acceptable given dramatic generation quality improvements (85--89\% OD match vs 12\%).

\subsubsection{Inference Speed Characteristics}

\textbf{Architectural equivalence:} Vanilla and distilled HOSER use identical architectures. The distillation loss ($\lambda \mathcal{L}_{\text{KL}}$) affects only training; at inference, both models execute the same forward pass with the same computational complexity. Therefore, inference speeds are identical by construction.

\textbf{Student vs Teacher comparison:} HOSER's zone-based architecture with spatial pruning enables substantially faster inference than LM-TAD's full-sequence transformer attention. While formal latency benchmarking remains future work (Section~\ref{sec:conclusion-future}), the architectural differences (hierarchical zones with candidate pruning vs quadratic attention) provide the theoretical foundation for speed advantages claimed in the methodology.

\subsubsection{Generation Throughput}

Trajectory generation uses beam search (width $b=4$) for both vanilla and distilled models. Generation speed depends on: (i) model forward pass latency, (ii) beam search overhead, (iii) candidate pruning efficiency. Since distilled and vanilla models share architecture, generation throughput is identical. The key result is that distillation improves \emph{generation quality} (OD match, JSD) without sacrificing generation speed.


