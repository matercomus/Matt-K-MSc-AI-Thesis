\begin{figure}[t]
    \centering
    \begin{tikzpicture}[
        % Styles
        node distance=2.2cm,
        box/.style={draw, thick, rectangle, rounded corners=5pt, minimum height=0.8cm, align=center, line width=0.8pt},
        student/.style={box, fill=green!12, draw=green!70, minimum width=2.8cm},
        loss/.style={box, fill=red!10, draw=red!60, minimum width=2.5cm},
        teacher/.style={box, fill=blue!10, draw=blue!60, dashed, minimum width=2.5cm},
        input/.style={box, fill=gray!10, draw=gray!60, minimum width=2.8cm},
        distillbox/.style={draw, dashed, thick, rounded corners=8pt, draw=orange!60},
        arrow/.style={->, thick, >=stealth},
        looparrow/.style={arrow, line width=1.8pt, draw=red!70},
        knowledge/.style={arrow, draw=purple!70},
        ]

        % Nodes
        \node[input] (input) {
            \textbf{Input Trajectory}\\
            {\footnotesize $\mathbf{r}_{1:t}$}
        };
        
        \node[student, above=2cm of input] (hoser) {
            \textbf{Student} $\mathcal{H}_\theta$\\
            {\tiny HOSER (trainable)}
        };
        
        \node[loss, above left=1.8cm and 1.5cm of hoser] (celoss) {
            $\mathcal{L}_{\text{CE}} + \alpha\mathcal{L}_{\text{time}}$\\
            {\tiny Supervised losses}
        };
        
        \node[loss, above right=1.5cm and 0.6cm of hoser] (kl) {
            $\mathcal{L}_{\text{KL}}^{(\tau)}$\\
            {\tiny Distillation}
        };
        
        \node[teacher, right=2cm of kl] (lmtad) {
            \textbf{Teacher} $\mathcal{L}_\phi$\\
            {\tiny LM-TAD (frozen)}
        };
        
        \node[loss, above=2.8cm of hoser, minimum width=3.5cm] (total) {
            \textbf{Total Loss}\\
            {\footnotesize $\mathcal{L} = \mathcal{L}_{\text{CE}} + \lambda \mathcal{L}_{\text{KL}}^{(\tau)} + \alpha \mathcal{L}_{\text{time}}$}
        };
        
        % Knowledge Distillation box
        \node[distillbox, fit=(kl)(lmtad), inner sep=0.3cm, label={[font=\small\bfseries, text=orange!80]above:Knowledge Distillation}] (kdbox) {};
        
        % Arrows with labels
        \draw[arrow] (input) -- node[right, font=\tiny] {$\psi(\mathbf{r})$} (hoser);
        
        \draw[arrow] (hoser.150) -- node[left, font=\tiny] {predictions} (celoss.south);
        
        \draw[arrow] (hoser.30) -- node[right, font=\tiny] {$p^{(\tau)}$} (kl.south);
        
        \draw[knowledge] (lmtad) -- node[above, font=\tiny] {$q^{(\tau)}$} (kl);
        
        \draw[arrow] (celoss) |- (total.200);
        \draw[arrow] (kl) |- (total.340);
        
        % Training loop - simpler arc
        \draw[looparrow, bend left=50] (total.west) to node[left, align=center] {
            \textbf{Training Loop}\\
            {\tiny $\theta \leftarrow \theta - \eta\nabla_\theta\mathcal{L}$}
        } (hoser.west);
        
        % Temperature scaling note
        \node[text=orange!70, font=\tiny, align=center] at ($(kl)+(0,-0.6)$) {
            $\tau$-scaled\\softmax
        };

    \end{tikzpicture}
    \caption{Knowledge distillation framework for trajectory prediction. The student model $\mathcal{H}_\theta$ (HOSER) learns from both supervised losses and knowledge distillation from the frozen teacher $\mathcal{L}_\phi$ (LM-TAD). Temperature scaling with parameter $\tau$ softens the probability distributions $q^{(\tau)}$ and $p^{(\tau)}$ to enable effective knowledge transfer. The training loop optimizes only the student parameters $\theta$ via gradient descent.}
    \label{fig:distillation-framework}
\end{figure}