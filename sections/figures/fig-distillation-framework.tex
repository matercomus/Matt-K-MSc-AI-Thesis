\begin{figure}[t]
    \centering
    \begin{tikzpicture}[
            % Styles
            node distance=2.2cm,
            box/.style={draw, thick, rectangle, rounded corners=5pt, minimum height=0.8cm, align=center, line width=0.8pt},
            student/.style={box, fill=green!12, draw=green!70, minimum width=2.8cm},
            loss/.style={box, fill=red!10, draw=red!60, minimum width=2.5cm},
            teacher/.style={box, fill=blue!10, draw=blue!60, minimum width=2.5cm},
            input/.style={box, fill=gray!10, draw=gray!60, minimum width=2.8cm},
            process/.style={box, fill=yellow!15, draw=orange!70, minimum width=2.2cm, font=\footnotesize},
            distillbox/.style={draw, dashed, thick, rounded corners=8pt, draw=orange!60},
            annotation/.style={draw=none, font=\scriptsize, text=black!70, align=center},
            badge/.style={draw=none, font=\tiny, text=black!60, align=center},
            % Arrow styles for different data types
            arrow/.style={->, thick, >=stealth},
            arrow_main/.style={->, very thick, >=stealth, line width=1.2pt},
            arrow_soft/.style={->, thick, >=stealth, densely dashed},
            arrow_loss/.style={->, very thick, >=stealth, draw=red!70},
            looparrow/.style={arrow, line width=1.8pt, draw=red!70, dashed, rounded corners=5pt},
            knowledge/.style={arrow, draw=purple!70, thick},
        ]

        % Bottom: Input (centered)
        \node[input] (input) {
            \textbf{Input Trajectory} $\mathbf{r}_{1:t}$
        };

        % Level 1: Models (reduced spacing for compactness)
        \node[student, above left=2cm and 0.5cm of input] (hoser) {
        \textbf{Student} $\mathcal{H}_\theta$\\
        {\small HOSER \emoji{fire}}
        };

        \node[teacher, above right=2cm and 0.5cm of input] (lmtad) {
        \textbf{Teacher} $\mathcal{L}_\phi$\\
        {\small LM-TAD \emoji{snowflake}}
        };

        % Level 2: Processing operations (adjusted to prevent overlap)
        \node[process, above=1cm of hoser, xshift=0cm] (candgen) {
        \textbf{Candidate Gen}\\
        {\scriptsize Graph pruning}\\
        {\scriptsize Top-$k$ → $\mathcal{C}_t$}
        };

        \node[process, above=1cm of hoser, xshift=4cm] (tempscale_student) {
        \textbf{Temp Scale}\\
        {\scriptsize softmax($\boldsymbol{\ell}^{\mathcal{H}}/\tau$)}\\
        {\scriptsize → $p^{(\tau)}$}
        };

        \node[process, above=1cm of lmtad] (renorm) {
        \textbf{Extract \& Renorm}\\
        {\scriptsize $\tilde{q}_t \to q_t^{(\tau)}$}\\
        {\scriptsize over $\mathcal{C}_t$}
        };

        % Level 3: Individual loss components
        \node[loss, above=1cm of candgen, minimum width=2cm] (celoss) {
        $\mathcal{L}_{\text{CE}}$\\
        {\tiny Cross-Entropy}
        };

        \node[loss, above=1cm of tempscale_student, xshift=0cm, minimum width=1.8cm] (timeloss) {
        $\mathcal{L}_{\text{time}}$\\
        {\tiny Time MAPE}
        };

        \node[loss, above=1cm of renorm, xshift=0cm, minimum width=1.8cm] (klloss) {
        $\mathcal{L}_{\text{KL}}^{(\tau)}$\\
        {\tiny Distillation}
        };

        % Level 4: Total loss (centered above loss components)
        \node[loss, above=1.4cm of timeloss, xshift=0cm, minimum width=4cm] (total) {
        \textbf{Total Loss}\\
        {\footnotesize $\mathcal{L} = \mathcal{L}_{\text{CE}} + \alpha \mathcal{L}_{\text{time}} + \lambda \tau^2 \mathcal{L}_{\text{KL}}$}
        };

        % Knowledge Distillation box (includes teacher, renorm, temp scale, and KL loss)
        \node[distillbox, fit=(renorm)(lmtad)(tempscale_student)(klloss), inner sep=0.4cm, inner ysep=0.5cm, label={[font=\small\bfseries, text=orange!80, rotate=90, anchor=mid, yshift=-0.5cm]east:Knowledge Distillation}] (kdbox) {};

        % Compact annotations
        \node[annotation, left=0.08cm of hoser.west, anchor=east] {$|\mathcal{V}|$};
        \node[annotation, right=0.08cm of lmtad.east, anchor=west] {$|\mathcal{Z}|$};
        \node[badge, below=0.08cm of input.south] {Inference: Student only};

        % Input to Models
        \draw[arrow_main] (input) -- node[left, font=\small] {roads} (hoser);
        \draw[arrow] (input) -- node[right, font=\small] {$\psi(\mathbf{r})$} (lmtad);
        \node[annotation, below=0.05cm of lmtad.south] {$w$-window};

        % Models to Processing
        \draw[arrow_main] (hoser) -- node[left, font=\scriptsize] {context} (candgen);
        \draw[arrow_main] (hoser) -- node[right, font=\scriptsize] {logits} (tempscale_student);
        \draw[knowledge] (lmtad) -- node[right, font=\scriptsize] {probs} (renorm);

        % Processing to Individual Losses (with required variables)
        \draw[arrow_main] (candgen) -- node[left, font=\tiny] {predictions, $y$} (celoss);
        \draw[arrow_main] (hoser) -- node[right, font=\tiny] {$\hat{t}$, $t$} (timeloss.south);
        \draw[arrow_soft] (tempscale_student) -- node[left, font=\tiny] {$p^{(\tau)}$} (klloss);
        \draw[arrow_soft] (renorm) -- node[right, font=\tiny] {$q^{(\tau)}$} (klloss);

        % Individual Losses to Total Loss
        \draw[arrow_loss] (celoss) -- node[left, font=\tiny] {} (total.south west);
        \draw[arrow_loss] (timeloss) -- node[left, font=\tiny] {$\alpha$} (total);
        \draw[arrow_loss] (klloss) -- node[right, font=\tiny] {$\lambda \tau^2$} (total.south east);


        % Training loop (simplified)
        \draw[looparrow] (total.west) -- ++(-4.5,0) |- (hoser.west) node[pos=0.3, xshift=-0.5cm, align=center, rotate=90, font=\footnotesize] {
                \textbf{Training}\\
                $\theta \leftarrow \theta - \eta\nabla_\theta\mathcal{L}$
            };

    \end{tikzpicture}
    \caption{Knowledge distillation framework with bottom-to-top data flow showing all loss computations. Input trajectories flow through two paths: (Left) Student path generates candidates via graph pruning and temperature-scales logits to $p^{(\tau)}$; (Right) Teacher path applies vocabulary mapping $\psi$, then extracts and renormalizes to $q^{(\tau)}$. Individual losses compute from their required inputs: $\mathcal{L}_{\text{CE}}$ from predictions and labels $y$, $\mathcal{L}_{\text{time}}$ from predicted/actual times $\hat{t}$/$t$, and $\mathcal{L}_{\text{KL}}$ from soft targets $p^{(\tau)}$ and $q^{(\tau)}$. Total loss aggregates components with weights $\alpha=0.1$ (time) and $\lambda$ (distillation, with $\tau^2$ scaling). Training loop updates student $\theta$ via gradient descent; teacher $\phi$ remains frozen. At inference, only student is deployed.}
    \label{fig:distillation-framework}
\end{figure}