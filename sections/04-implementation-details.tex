% Source: Moved from sections/03-methodology.tex (ยง3.3.2, ยง3.4, ยง3.5)
% Additional content from notes/hoser/docs/DATASET_SETUP.md

\section{Implementation}
\label{sec:implementation}

This section describes the practical implementation of our knowledge distillation framework, covering dataset preparation, hyperparameter tuning, training optimizations, and computational infrastructure. These implementation choices enable efficient training on commodity hardware while maintaining reproducibility.

\subsection{Codebase and Modifications}
\label{sec:impl-codebase}

Our experiments build upon the official implementations of both LM-TAD~\cite{mbuyaTrajectoryAnomalyDetection2024,JonathankabalaLMTAD} and HOSER~\cite{caoHolisticSemanticRepresentation2025,HOSEREvaluationMainipynb}. We modified both codebases to integrate the distillation framework while preserving the original model architectures and hyperparameters from their respective papers. Both vanilla HOSER (baseline) and distilled HOSER (proposed) variants run on our unified modified implementation to ensure fair comparison---the distillation loss ($\lambda \mathcal{L}_{\text{KL}}$) is the only difference between the two configurations. This controlled setup isolates the impact of knowledge transfer from other implementation factors.

Additional engineering optimizations include: replacing HOSER's A* greedy search with beam search for trajectory generation (improved efficiency), vectorizing data collation functions, and adding WandB integration for experiment tracking. These modifications improve computational performance without affecting model behavior or evaluation fairness.

\subsection{Dataset Preparation Pipeline}
\label{sec:impl-dataset-prep}

The distillation framework requires preprocessing to bridge HOSER's road-based and LM-TAD's grid-based representations. We use the Beijing dataset (\autoref{sec:data-overview}) from the original HOSER paper~\cite{caoHolisticSemanticRepresentation2025}.

\subsubsection{Road Network Preprocessing}

The student model operates on hierarchical zones as specified in the original HOSER paper~\cite{caoHolisticSemanticRepresentation2025}. Zone partitioning and transition matrix construction follow the procedures detailed in \autoref{sec:data-pipeline}.

\subsubsection{Teacher Model Preparation}

The LM-TAD teacher model is loaded from pre-trained checkpoints with vocabulary mapping $\psi$ (\autoref{def:vocab-mapping}) precomputed and cached for efficient training. Checkpoint conversion details are provided in \autoref{sec:data-pipeline}.

\subsection{Hyperparameter Optimization}
\label{sec:impl-hparam}

We employ a systematic two-phase hyperparameter search using the Optuna framework~\cite{akibaOptunaNextgenerationHyperparameter2019} to identify optimal distillation parameters.

The balance between distillation weight and temperature is non-trivial and task-dependent. Recent work demonstrates that minimal distillation weights can be effective when paired with appropriate temperature scaling: AdaSwitch~\cite{pengAdaSwitchAdaptiveSwitching2025} shows that minimal teacher intervention preserves student capacity, while ORPO-Distill~\cite{singhORPODistillMixedPolicyPreference2025} demonstrates that excessive supervision can narrow output distributions. These findings support systematic hyperparameter search rather than assuming universal optimal values.

\subsubsection{Search Space Design}

Three hyperparameters govern knowledge transfer (\hyperref[app:hyperparam-space]{Table~\ref*{tab:hyperparam-search-appendix}, Appendix~\ref*{app:hyperparam-space}}): $\lambda$ (distillation weight, log scale [0.001, 0.1]) controls teacher influence vs. supervised signal; $\tau$ (temperature, linear [1.0, 5.0]) smooths distributions to expose ``dark knowledge''; and $w$ (window size, [2, 8]) determines teacher context length, trading accuracy for speed.

\subsubsection{Two-Phase Optimization Strategy}

\textbf{Phase 1: Exploration (12 trials).} We employ CMA-ES (Covariance Matrix Adaptation Evolution Strategy)~\cite{hansenCMAEvolutionStrategy2023} as the sampler, which efficiently navigates continuous parameter spaces. The Hyperband pruner~\cite{liHyperbandNovelBanditBased2018} with minimum resource allocation of 5 epochs terminates unpromising configurations early.

Key configuration:
\begin{itemize}[noitemsep,topsep=0pt]
    \item \textbf{Objective}: Maximize validation accuracy after 8 epochs
    \item \textbf{Baseline}: Trial 0 always runs vanilla training ($\lambda = 0$) for fair comparison
    \item \textbf{Budget}: 12 trials $\times$ 8 epochs = 96 training runs (pruning reduces actual compute)
\end{itemize}

\textbf{Phase 2: Validation (3 seeds).} The best configuration from Phase 1 is trained to completion (25 epochs) with three random seeds $\{42, 43, 44\}$ to assess robustness and estimate variance.

\subsubsection{Optimal Configuration}

The hyperparameter search identifies dataset-specific optimal values that demonstrate the necessity of per-dataset tuning.

\paragraph{Beijing Dataset}
A 12-trial CMA-ES optimization identified optimal values:
\begin{itemize}[leftmargin=*,noitemsep]
    \item $\lambda = 0.0014$ (KL divergence weight)
    \item $\tau = 4.37$ (temperature)
    \item $w = 7$ (teacher context window, steps)
\end{itemize}
These hyperparameters achieve 57.24\% validation accuracy (marginal +0.01\% over vanilla's 57.23\%), yet produce dramatic generation quality improvements (85--89\% OD match rate vs vanilla's 12\%), as detailed in Section~\ref{sec:eval-beijing}. This disconnect between validation and generation metrics indicates that next-step prediction accuracy is a poor proxy for long-horizon trajectory realism, motivating multi-objective optimization approaches discussed in Section~\ref{sec:conclusion-future}.

\paragraph{Porto Dataset}
A 2-phase optimization (Phase 1: 20 trials broad exploration, Phase 2: 20 trials narrow refinement) identified:
\begin{itemize}[leftmargin=*,noitemsep]
    \item \textbf{Phase 1}: $\lambda = 0.00644$, $\tau = 2.802$, $w = 4$ (validation acc 26.53\%, used for evaluation in Section~\ref{sec:eval-porto})
    \item \textbf{Phase 2}: $\lambda = 0.00598$, $\tau = 2.515$, $w = 4$ (validation acc 26.55\%, +0.02\% improvement, full evaluation pending)
\end{itemize}

\textbf{Cross-dataset divergence}: Porto optimal hyperparameters differ significantly from Beijing---$\lambda$ is 4.3$\times$ higher (0.00644 vs 0.0014), $\tau$ is 42\% lower (2.802 vs 4.37), and $w$ is 43\% shorter (4 vs 7). This divergence demonstrates that cross-dataset hyperparameter transfer fails and per-dataset HPO studies are mandatory. The higher Porto $\lambda$ is counterintuitive given the easier task (vanilla achieves 88.8\% OD match vs Beijing's 12\%), potentially attributable to batch size reduction (32 vs 128) due to memory constraints with longer trajectories---an uncontrolled confound discussed in Section~\ref{sec:conclusion-future}.

\begin{table}[h]
    \centering
    \caption{Optimal distillation hyperparameters from Optuna tuning}
    \label{tab:optimal-hparams}
    \small
    \begin{tabular}{lll}
        \toprule
        \textbf{Parameter} & \textbf{Optimal Value} & \textbf{Interpretation}                     \\
        \midrule
        $\lambda$          & 0.0014                 & Very subtle teacher guidance                \\
        $\tau$             & 4.37                   & High temperature (broad knowledge transfer) \\
        $w$                & 7                      & Nearly full context window                  \\
        \bottomrule
    \end{tabular}
\end{table}

This configuration suggests that \emph{subtle distributional guidance} from the teacher, rather than aggressive knowledge transfer, enables the student to integrate spatial understanding without compromising its architectural strengths.

\subsection{Training Optimizations}
\label{sec:impl-opt}

To enable practical training on commodity GPU hardware, we implement several key optimizations that reduce memory consumption and accelerate training.

\subsubsection{Automatic Mixed Precision}

We employ PyTorch's Automatic Mixed Precision (AMP) framework to reduce memory footprint:

\begin{itemize}[noitemsep,topsep=0pt]
    \item \textbf{Teacher inference}: FP16 precision with \texttt{autocast} context reduces memory by $\sim$50\% with negligible accuracy loss
    \item \textbf{Student training}: TF32 format for matrix operations maintains numerical stability
    \item \textbf{Gradient scaling}: Dynamic loss scaling prevents underflow in FP16 backward passes
\end{itemize}

The frozen teacher benefits most from FP16 inference, as it performs no gradient updates and requires only forward-pass accuracy.

\subsubsection{Memory Management}

\textbf{Intelligent Caching.} The framework automatically decides whether to cache the dataset in RAM based on available memory. For the Beijing dataset ($\sim$630k trajectories, $\sim$13GB), RAM caching eliminates disk I/O bottlenecks when sufficient memory is available. The dataset can be loaded entirely into RAM alongside model and optimizer states. Otherwise, the system streams data from NVMe storage with minimal performance degradation.

\textbf{Gradient Accumulation.} We simulate an effective batch size of 512 by accumulating gradients over 8 micro-batches of 64 samples each. This enables large-batch training benefits while respecting GPU memory constraints.

\textbf{Candidate Filtering.} HOSER's spatial pruning limits the candidate set to $k = 64$ nearest roads per timestep, reducing the output dimensionality from $|\mathcal{V}| = 40{,}060$ to a manageable subset.

\subsubsection{Batched Operations}

All vocabulary mapping and teacher inference operations are fully vectorized:

\begin{itemize}[noitemsep,topsep=0pt]
    \item \textbf{Road-to-grid mapping}: GPU-accelerated tensor operations via precomputed lookup tables
    \item \textbf{Label remapping}: Parallel remapping with masked positions set to $-100$ (ignored by loss)
    \item \textbf{Teacher inference}: Batch-wise forward pass processes all timesteps simultaneously
\end{itemize}

These optimizations yield 11--13 iterations/second training throughput. Crucially, teacher inference adds less than 2\% overhead compared to vanilla HOSER training, making distillation nearly cost-free.

\subsection{Training Infrastructure}
\label{sec:impl-infra}

\textbf{Hardware:} NVIDIA RTX 2080 Ti GPU with 64GB system RAM (\hyperref[app:training-config]{Appendix~\ref*{app:training-config}} for complete specifications).

\subsubsection{Training Configuration}

We use AdamW optimizer with cosine annealing and effective batch size 512 (64 samples $\times$ 8 accumulation steps). Training runs 25 epochs ($\sim$36 hours) with fixed seeds (42, 43, 44) for reproducibility. Full configuration details are provided in \hyperref[app:training-config]{Table~\ref*{tab:training-config-appendix}, Appendix~\ref*{app:training-config}}.

\subsection{Practical Considerations}
\label{sec:impl-practical}

\subsubsection{Dataset-Specific Adaptations}

Different urban networks require minor adaptations:

\textbf{Beijing}: Standard configuration with 1024 max trajectory length.

\textbf{Porto}: Longer trajectories (avg. 8 vs 4.6 points) require reduced batch size (64 $\rightarrow$ 32) and gradient checkpointing to prevent memory overflow. Memory usage scales quadratically with trajectory length due to attention mechanisms and distance matrices.

\textbf{BJUT (Private Beijing)}: Similar to Beijing reference dataset but requires independent map-matching and preprocessing pipeline.

\subsubsection{Common Challenges and Solutions}

\textbf{Grid dimension mismatch}: Ensure LM-TAD's \texttt{grip\_size} parameter matches the vocabulary mapping configuration. Beijing uses $205 \times 252$ grid, Porto uses $46 \times 134$.

\textbf{Highway type parsing}: Some datasets use nested list formats (e.g., \texttt{["primary", "secondary"]}), others use integer codes. The data loader handles both via conditional parsing.

\textbf{Memory overflow}: Reduce batch size or enable gradient checkpointing for datasets with longer trajectories. Porto requires 50\% smaller batches than Beijing despite similar dataset sizes.

\textbf{Reproducibility:} Complete implementation, trained models, evaluation scripts, and configuration files are provided in the supplementary materials to facilitate reproduction of all experiments.

