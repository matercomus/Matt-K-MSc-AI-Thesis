% Source: Moved from sections/03-methodology.tex (ยง3.3.2, ยง3.4, ยง3.5)
% Additional content from notes/hoser/docs/DATASET_SETUP.md

\section{Implementation}
\label{sec:implementation}

This section describes the practical implementation of our knowledge distillation framework, covering dataset preparation, hyperparameter tuning, training optimizations, and computational infrastructure. These implementation choices enable efficient training on commodity hardware while maintaining reproducibility.

\subsection{Dataset Preparation Pipeline}
\label{sec:impl-dataset-prep}

The distillation framework requires preprocessing to bridge HOSER's road-based and LM-TAD's grid-based representations. We use the Beijing dataset (\autoref{sec:data-overview}) from the original HOSER paper~\cite{caoHolisticSemanticRepresentation2025}.

\subsubsection{Road Network Preprocessing}

The student model operates on hierarchical zones as specified in the original HOSER paper~\cite{caoHolisticSemanticRepresentation2025}. Zone partitioning and transition matrix construction follow the procedures detailed in \autoref{sec:data-pipeline}.

\subsubsection{Teacher Model Preparation}

The LM-TAD teacher model is loaded from pre-trained checkpoints with vocabulary mapping $\phi$ (\autoref{sec:method-vocab}) precomputed and cached for efficient training. Checkpoint conversion details are provided in \autoref{sec:data-pipeline}.

\subsection{Hyperparameter Optimization}
\label{sec:impl-hparam}

We employ a systematic two-phase hyperparameter search using the Optuna framework~\cite{akibaOptunaNextgenerationHyperparameter2019} to identify optimal distillation parameters.

\subsubsection{Search Space Design}

Three hyperparameters govern knowledge transfer (\hyperref[app:hyperparam-space]{Table~\ref*{tab:hyperparam-search-appendix}, Appendix~\ref*{app:hyperparam-space}}): $\lambda$ (distillation weight, log scale [0.001, 0.1]) controls teacher influence vs. supervised signal; $\tau$ (temperature, linear [1.0, 5.0]) smooths distributions to expose ``dark knowledge''; and $w$ (window size, [2, 8]) determines teacher context length, trading accuracy for speed.

\subsubsection{Two-Phase Optimization Strategy}

\textbf{Phase 1: Exploration (12 trials).} We employ CMA-ES (Covariance Matrix Adaptation Evolution Strategy)~\cite{hansenCMAEvolutionStrategy2023} as the sampler, which efficiently navigates continuous parameter spaces. The Hyperband pruner~\cite{liHyperbandNovelBanditBased2018} with minimum resource allocation of 5 epochs terminates unpromising configurations early.

Key configuration:
\begin{itemize}[noitemsep,topsep=0pt]
\item \textbf{Objective}: Maximize validation accuracy after 8 epochs
\item \textbf{Baseline}: Trial 0 always runs vanilla training ($\lambda = 0$) for fair comparison
\item \textbf{Budget}: 12 trials $\times$ 8 epochs = 96 training runs (pruning reduces actual compute)
\end{itemize}

\textbf{Phase 2: Validation (3 seeds).} The best configuration from Phase 1 is trained to completion (25 epochs) with three random seeds $\{42, 43, 44\}$ to assess robustness and estimate variance.

\subsubsection{Optimal Configuration}

The hyperparameter search yields an unexpected result: minimal distillation weight with high temperature proves most effective.

\begin{table}[h]
\centering
\caption{Optimal distillation hyperparameters from Optuna tuning}
\label{tab:optimal-hparams}
\small
\begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{Optimal Value} & \textbf{Interpretation} \\
\midrule
$\lambda$ & 0.0014 & Very subtle teacher guidance \\
$\tau$ & 4.37 & High temperature (broad knowledge transfer) \\
$w$ & 7 & Nearly full context window \\
\bottomrule
\end{tabular}
\end{table}

This configuration suggests that \emph{subtle distributional guidance} from the teacher, rather than aggressive knowledge transfer, enables the student to integrate spatial understanding without compromising its architectural strengths.

\subsection{Training Optimizations}
\label{sec:impl-opt}

To enable practical training on commodity GPU hardware, we implement several key optimizations that reduce memory consumption and accelerate training.

\subsubsection{Automatic Mixed Precision}

We employ PyTorch's Automatic Mixed Precision (AMP) framework to reduce memory footprint:

\begin{itemize}[noitemsep,topsep=0pt]
\item \textbf{Teacher inference}: FP16 precision with \texttt{autocast} context reduces memory by $\sim$50\% with negligible accuracy loss
\item \textbf{Student training}: TF32 format for matrix operations maintains numerical stability
\item \textbf{Gradient scaling}: Dynamic loss scaling prevents underflow in FP16 backward passes
\end{itemize}

The frozen teacher benefits most from FP16 inference, as it performs no gradient updates and requires only forward-pass accuracy.

\subsubsection{Memory Management}

\textbf{Intelligent Caching.} The framework automatically decides whether to cache the dataset in RAM based on available memory. For the Beijing dataset ($\sim$630k trajectories, $\sim$13GB), RAM caching eliminates disk I/O bottlenecks when sufficient memory is available. The dataset can be loaded entirely into RAM alongside model and optimizer states. Otherwise, the system streams data from NVMe storage with minimal performance degradation.

\textbf{Gradient Accumulation.} We simulate an effective batch size of 512 by accumulating gradients over 8 micro-batches of 64 samples each. This enables large-batch training benefits while respecting GPU memory constraints.

\textbf{Candidate Filtering.} HOSER's spatial pruning limits the candidate set to $k = 64$ nearest roads per timestep, reducing the output dimensionality from $|\mathcal{V}| = 40{,}060$ to a manageable subset.

\subsubsection{Batched Operations}

All vocabulary mapping and teacher inference operations are fully vectorized:

\begin{itemize}[noitemsep,topsep=0pt]
\item \textbf{Road-to-grid mapping}: GPU-accelerated tensor operations via precomputed lookup tables
\item \textbf{Label remapping}: Parallel remapping with masked positions set to $-100$ (ignored by loss)
\item \textbf{Teacher inference}: Batch-wise forward pass processes all timesteps simultaneously
\end{itemize}

These optimizations yield 11--13 iterations/second training throughput. Crucially, teacher inference adds less than 2\% overhead compared to vanilla HOSER training, making distillation nearly cost-free.

\subsection{Training Infrastructure}
\label{sec:impl-infra}

\textbf{Hardware:} NVIDIA RTX 2080 Ti GPU with 64GB system RAM (\hyperref[app:training-config]{Appendix~\ref*{app:training-config}} for complete specifications).

\subsubsection{Training Configuration}

We use AdamW optimizer with cosine annealing and effective batch size 512 (64 samples $\times$ 8 accumulation steps). Training runs 25 epochs ($\sim$36 hours) with fixed seeds (42, 43, 44) for reproducibility. Full configuration details are provided in \hyperref[app:training-config]{Table~\ref*{tab:training-config-appendix}, Appendix~\ref*{app:training-config}}.

\subsection{Practical Considerations}
\label{sec:impl-practical}

\subsubsection{Dataset-Specific Adaptations}

Different urban networks require minor adaptations:

\textbf{Beijing}: Standard configuration with 1024 max trajectory length.

\textbf{Porto}: Longer trajectories (avg. 8 vs 4.6 points) require reduced batch size (64 $\rightarrow$ 32) and gradient checkpointing to prevent memory overflow. Memory usage scales quadratically with trajectory length due to attention mechanisms and distance matrices.

\textbf{BJUT (Private Beijing)}: Similar to Beijing reference dataset but requires independent map-matching and preprocessing pipeline.

\subsubsection{Common Challenges and Solutions}

\textbf{Grid dimension mismatch}: Ensure LM-TAD's \texttt{grip\_size} parameter matches the vocabulary mapping configuration. Beijing uses $205 \times 252$ grid, Porto uses $46 \times 134$.

\textbf{Highway type parsing}: Some datasets use nested list formats (e.g., \texttt{["primary", "secondary"]}), others use integer codes. The data loader handles both via conditional parsing.

\textbf{Memory overflow}: Reduce batch size or enable gradient checkpointing for datasets with longer trajectories. Porto requires 50\% smaller batches than Beijing despite similar dataset sizes.

\textbf{Reproducibility:} Complete implementation, trained models, evaluation scripts, and configuration files are provided in the supplementary materials to facilitate reproduction of all experiments.

