\section{Experimental Setup and Results}
\label{sec:evaluation}

The evaluation framework is designed to be comprehensive and multi-faceted, assessing the generated data from three critical perspectives: the performance of the anomaly detection system, the quality of the synthetic data, and the robustness of the privacy-preserving mechanisms.

\subsection{Experimental Design and Validation Strategy}
\label{sec:exp-design}

The experimental design implements a per-city pipeline validation strategy to assess framework generalizability and robustness across diverse urban environments.

\begin{description}
  \item[Primary Framework Development] The complete three-phase framework (baseline generation, anomaly mining, iterative refinement) is developed and optimized using the Beijing taxi dataset. This provides a comprehensive implementation baseline with well-characterized performance metrics.
  \item[Independent Pipeline Replication] To assess generalizability, the entire framework pipeline is independently executed on additional datasets from Chengdu and Xi'an. Each city receives separate model training, privacy budget allocation, and iterative refinement cycles, ensuring fair comparison across urban environments with different characteristics.
  \item[Cross-City Performance Analysis] Framework performance is systematically compared across cities using identical evaluation metrics, enabling assessment of: (1) baseline synthetic data quality across different urban contexts, (2) anomaly detection effectiveness in diverse mobility patterns, and (3) privacy protection consistency across varied trajectory characteristics.
  \item[Public vs. Private Dataset Validation] The framework includes comparative analysis between public and private datasets from Beijing to investigate potential discrepancies and biases in publicly available data, ensuring robust evaluation foundations.
\end{description}

\subsection{Anomaly Detection Performance}
\label{sec:results}

The performance of the anomaly detection system is evaluated using metrics appropriate for imbalanced datasets, where anomalies are rare.

\begin{description}
  \item[Key Performance Metrics] Evaluation focuses on Precision, Recall, and the F1-Score, which provide a balanced view of the detector's ability to correctly identify rare anomalous instances. These metrics are standard in the field and are used in comparable studies involving language model-based anomaly detection~\cite{mbuyaTrajectoryAnomalyDetection2024}.
  \item[AUC-ROC and AUC-PR] The Area Under the Receiver Operating Characteristic Curve (AUC-ROC) and the Area Under the Precision-Recall Curve (AUC-PR) are used to assess the model's overall discriminative power, with AUC-PR being particularly informative for imbalanced class distributions.
\end{description}

\subsection{Synthetic Data Quality Evaluation}
\label{sec:synthetic-eval}

The quality of the generated synthetic data is assessed using a standardized framework to ensure it is both realistic and useful for downstream tasks.

\begin{description}
  \item[Standardized Quality Assessment] The \textbf{SDMetrics} library is employed to systematically evaluate the synthetic data. This includes assessing statistical resemblance to real data (Resemblance), utility for machine learning tasks (Utility), and protection against disclosure (Privacy).
  \item[Statistical Fidelity] Distribution comparison tests (e.g., Kolmogorov-Smirnov, Jensen-Shannon divergence) are used to quantitatively measure the statistical similarity between the real and synthetic trajectory data distributions for key properties like trip duration and distance, an approach consistent with the evaluation in~\cite{zhuDiffTrajGeneratingGPS2023}.
  \item[Downstream Task Performance] The utility of the synthetic data is further validated by evaluating the performance of downstream models (e.g., travel time estimation, destination prediction, route classification) trained on the synthetic data versus models trained on real data. This evaluation approach is consistent with the utility assessment strategy in~\cite{zhuDiffTrajGeneratingGPS2023}, where synthetic data utility was demonstrated through downstream prediction tasks.
\end{description}

\subsection{Privacy Preservation Assessment}
\label{sec:privacy-eval}

The privacy guarantees of the synthetic data are evaluated through a series of attack simulations designed to test its resilience against re-identification.

\begin{description}
  \item[Membership Inference Attacks] Tests are conducted to determine whether an adversary can successfully identify whether a specific, real trajectory was part of the original training dataset used to create the synthetic data.
  \item[Trajectory Reconstruction Attacks] The framework is evaluated on its ability to prevent an adversary from reconstructing individual, real-world trajectories from the synthetic dataset.
  \item[Standardized Privacy Attack Evaluation using scikit-mobility] The privacy resilience of synthetic trajectories is systematically evaluated using the scikit-mobility framework's comprehensive attack suite. This includes location-based attacks (LocationAttack for general location inference, UniqueLocationAttack for unique location identification), frequency-based attacks (LocationFrequencyAttack, LocationProbabilityAttack, LocationProportionAttack), sequential pattern attacks (LocationSequenceAttack), temporal correlation attacks (LocationTimeAttack), and context-specific attacks (HomeWorkAttack for home/work location inference). Each attack class provides standardized \texttt{assess\_risk()} methods that quantify privacy vulnerabilities, enabling consistent and reproducible privacy evaluation across different synthetic data generation approaches.
  \item[Privacy-Utility Trade-off] A quantitative analysis is performed to measure the balance between the level of privacy protection achieved and the resulting utility of the data for anomaly detection research.
\end{description}

\subsection{Computational Performance Analysis}
\label{sec:performance}

\begin{compactoutline}
  \outlineitem{Scalability Analysis -- Performance with varying dataset sizes}
  \outlineitem{Resource Requirements -- Memory, CPU, time complexity analysis}
\end{compactoutline}

\subsection{Ablation Study}
\label{sec:ablation}

To quantify the impact of key components in the framework, we plan to perform two ablation experiments:
\begin{itemize}
  \item \textbf{No Rule-Based Curation:} We expect that removing the rule-based curation step and relying solely on the unsupervised detector will lead to a drop in anomaly detection F1-score (e.g., from 0.81 to 0.74), and the proportion of interpretable anomalies is expected to decrease significantly (e.g., from 92\% to 61\%).
  \item \textbf{No Iterative Refinement:} We anticipate that using a single-pass (non-iterative) approach will reduce the number of unique anomaly categories generated (e.g., from 6 to 3), and the overall anomaly detection F1-score is expected to drop (e.g., from 0.81 to 0.76).
\end{itemize}
These anticipated results would demonstrate that both rule-based curation and iterative refinement are critical for achieving high anomaly interpretability, diversity, and detection performance.
