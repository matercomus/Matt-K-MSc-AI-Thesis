\section{Related Work}
\label{sec:lit-review}

This section reviews the key research areas relevant to knowledge distillation for trajectory prediction. We examine trajectory prediction and generation methods, synthetic data applications for urban systems, trajectory anomaly detection approaches that learn spatial patterns, knowledge distillation techniques, the architectural foundations in graph neural networks and transformers, and cross-task knowledge transfer methods.

% ===== BEGIN REWRITTEN BODY (chronological narrative) =====

% Overview paragraph
The evolution of data‐driven trajectory research mirrors the broader progression of sequential modelling in artificial intelligence.  Beginning with recurrent neural networks and hand‐crafted statistical baselines, the field has steadily adopted richer spatial representations, more expressive sequence architectures and, most recently, cross–task knowledge transfer.  The subsections below follow this chronological arc, critiquing the advantages and limitations of each paradigm to motivate the distillation framework presented in~\autoref{sec:methodology}.

\subsection{Classical Trajectory Prediction Models}
\label{sec:lit-traj-pred}
Early work framed next–location prediction as a sequence modelling problem amenable to recurrent neural networks (RNNs) and their gated variants.  Memory‐augmented LSTMs~\cite{liang2018memory} and variational autoencoders~\cite{liuOnlineAnomalousTrajectory2020} captured short-range dependencies and route uncertainty, while GAN‐based approaches such as LSTM-TrajGAN~\cite{raoLSTMTrajGANDeepLearning2020} attempted adversarially faithful path synthesis.  These models established the feasibility of learning spatial–temporal patterns directly from GPS data, but their limited receptive field and difficulty handling map constraints curbed real-world adoption.  \textit{Takeaway}: classical RNN/GAN models prove the concept yet struggle with long-range coherence and graph topology.

\subsection{Graph Neural Networks for Road Networks}
\label{sec:lit-gnn}
The introduction of graph neural networks (GNNs)~\cite{kipfSemisupervisedClassificationGraph2017,veličkovićGraphAttentionNetworks2018} provided an explicit inductive bias for road topology.  By representing intersections and road segments as nodes and edges, GCNs aggregated neighbourhood information, whereas GATs learned edge-specific attention weights, supporting fine-grained routing decisions.  Hierarchical hybrids further combined local and regional reasoning.  Despite clear spatial benefits, pure GNN solutions often incurred high inference latency on large urban graphs.  \textit{Takeaway}: GNNs embed topology elegantly but computational cost motivates search for lighter yet expressive alternatives.

\subsection{Transformer Architectures for Mobility Sequences}
\label{sec:lit-transformer}
Transformers~\cite{vaswaniAttentionAllYou2023} revolutionised sequential learning through self-attention.  Mobility research quickly adopted this paradigm—LM-TAD treats trajectories as token sequences, achieving state-of-the-art anomaly detection accuracy.  Large language model (LLM) adaptations such as PathGen-LLM~\cite{liPathGenLLMLargeLanguage} demonstrated zero-shot path generalisation.  However, the quadratic cost of self-attention renders vanilla transformers impractical for real-time traffic services.  \textit{Takeaway}: transformers learn rich global dependencies but impose prohibitive inference overhead.

\subsection{Deep Generative Approaches: VAEs to Diffusion and LLMs}
\label{sec:lit-generative}
Parallel to architectural advances, generative modelling transitioned from VAEs to diffusion processes and LLM-style decoders.  Diffusion-based TrajGDM~\cite{chuSimulatingHumanMobility2024} reframed generation as uncertainty reduction, yielding diverse and realistic paths.  TrajGPT~\cite{hsuTrajGPTControlledSynthetic2024} leveraged transformers for controllable synthesis, while foundation-scale models integrated multi-modal context~\cite{maLearningUniversalHuman2025}.  These methods improved fidelity but further increased computational and data demands.  \textit{Takeaway}: modern generative models capture complex mobility patterns yet exacerbate scalability concerns.

% ---- Road-so-far synthesis ----
\paragraph{Road so far.}  Classical RNNs established learnability, GNNs injected topology awareness, transformers unlocked long-range context, and diffusion/LLM generators raised realism.  Nevertheless, none reconcile spatial expressiveness with the latency constraints of traffic operations.

\subsection{Synthetic Trajectory Generation for Urban Applications}
\label{sec:lit-synthetic-urban}
Recent literature positions synthetic data as a utility-centric asset for simulation and policy rather than solely for privacy.  SynMob~\cite{zhuSynMobCreatingHighFidelity} and related frameworks retain geo-statistical properties critical to urban planning.  This shift underlines the importance of high-quality generation that scales across cities, a requirement echoed by foundation mobility models~\cite{maLearningUniversalHuman2025}.  \textit{Takeaway}: urban stakeholders demand scalable, high-fidelity synthesis, heightening the need for efficient yet accurate predictors.

\subsection{Trajectory Anomaly Detection and Spatial Learning}
\label{sec:lit-anomaly-spatial}
Anomaly detectors such as LM-TAD learn what \emph{normal} mobility looks like by modelling probability distributions over location tokens~\cite{heSpatiotemporalTrajectoryAnomaly2022,kongMobileTrajectoryAnomaly2024}.  Their spatial insight is invaluable, yet transformer-based detectors are slower than prediction-oriented models like HOSER.  \textit{Takeaway}: anomaly detection encodes rich spatial priors that remain untapped by fast predictors.

\subsection{Knowledge Distillation and Model Compression}
\label{sec:lit-distill}
Knowledge distillation~\cite{hintonDistillingKnowledgeNeural2015} addresses the accuracy–efficiency dilemma by transferring soft targets from a high-capacity teacher to a lightweight student.  Vision and NLP studies show that students can approximate teachers with negligible runtime overhead.  \textit{Takeaway}: distillation offers a principled route to inherit transformer knowledge without paying inference cost.

\subsection{Cross-Task Transfer and Foundation Mobility Models}
\label{sec:lit-transfer}
Cross-task transfer extends distillation across objectives—e.g.
leveraging anomaly-detection priors to boost prediction~\cite{maLearningUniversalHuman2025}.  Foundation mobility models exemplify multi-domain knowledge sharing, yet concrete methods for teacher–student bridging remain under-explored.  \textit{Takeaway}: transferring spatial knowledge across tasks is promising but lacks systematic methodologies.

% ---- Second Road-so-far synthesis ----
\paragraph{Road so far.}  The literature converges on two complementary insights: (i) transformers learn superior spatial representations, and (ii) operational systems require millisecond-scale inference.

\subsection*{Synthesis and Motivation for This Thesis}
\label{sec:lit-synthesis}
Bridging these insights, our work distils the transformer-based LM-TAD anomaly detector into the graph-aware, low-latency HOSER predictor during training only, as detailed in~\autoref{sec:methodology}. This approach inherits rich spatial priors while preserving real-time performance, directly addressing the shortcomings identified above.

% ===== END REWRITTEN BODY =====
