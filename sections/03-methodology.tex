\section{Methodology}
\label{sec:methodology}

This section presents our knowledge distillation framework for transferring spatial reasoning from a trajectory anomaly detector to a fast route predictor. Figure~\ref{fig:distillation-framework} illustrates the complete pipeline, showing how the frozen teacher and trainable student interact through vocabulary mapping and temperature-scaled distributions. We detail the vocabulary alignment mechanism (\autoref{sec:method-vocab}), the temperature-scaled distillation objective (\autoref{sec:method-distill}), the optimized training pipeline (\autoref{sec:method-training}), and the inference procedure (\autoref{sec:method-inference}).

\subsection{Preliminaries}
\label{sec:method-prelim}

\subsubsection{Notation}
Table~\ref{tab:notation} summarizes the mathematical notation used throughout this section.

\begin{table}[h]
\centering
\caption{Mathematical notation}
\label{tab:notation}
\small
\begin{tabular}{ll}
\toprule
\textbf{Symbol} & \textbf{Description} \\
\midrule
$\mathcal{G} = (\mathcal{V}, \mathcal{E})$ & Road network graph with $|\mathcal{V}|$ segments \\
$\mathbf{r}_{1:t} = (r_1, \ldots, r_t)$ & Partial trajectory of $t$ road segments \\
$\mathcal{C}_t \subseteq \mathcal{V}$ & Candidate set at timestep $t$ (top-$k$ roads) \\
$\mathcal{Z} = \{1, \ldots, V\}$ & Grid cell vocabulary, $V = 51{,}663$ \\
$\phi: \mathcal{V} \rightarrow \mathcal{Z}$ & Road-to-grid mapping function \\
$q_t^{(\tau)}$ & Teacher distribution (temperature $\tau$) \\
$p_t^{(\tau)}$ & Student distribution (temperature $\tau$) \\
$\lambda$ & Distillation loss weight \\
$\tau$ & Temperature parameter \\
$w$ & History window size \\
$k$ & Candidate set size (typically 64) \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Problem Definition}
\label{sec:method-problem}

\begin{definition}[Map-Matched Trajectory Prediction]
Given a partial trajectory $\mathbf{r}_{1:t}$ on road network $\mathcal{G}$, a destination road $r_d \in \mathcal{V}$, and a candidate set $\mathcal{C}_t$ generated by spatial pruning, predict the probability distribution $p(r_{t+1} | \mathbf{r}_{1:t}, r_d, \mathcal{C}_t)$ over next road segments such that inference time satisfies $t_{\text{inf}} < 15$ ms.
\end{definition}

\subsubsection{Model Specifications}
\label{sec:method-models}

\textbf{Teacher (LM-TAD):} A pre-trained transformer for trajectory anomaly detection with:
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item Input: Grid cell sequences $\mathbf{z}_{1:t}$ where $z_i \in \mathcal{Z}$
\item Architecture: 6-layer causal transformer, 512-dim embeddings, 8 attention heads
\item Output: Next-token distribution $q(z_{t+1} | \mathbf{z}_{1:t})$ over $V$ grid cells
\item Inference speed: $\sim$430 ms/batch (frozen during distillation)
\end{itemize}

\textbf{Student (HOSER):} A hierarchical graph-aware predictor with:
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item Input: Road sequences $\mathbf{r}_{1:t}$ where $r_i \in \mathcal{V}$
\item Architecture: Zone embeddings (300) + 2-layer GCN + attention navigator
\item Output: Logits $\ell_t \in \mathbb{R}^{|\mathcal{C}_t|}$ for candidate roads
\item Inference speed: $\sim$13 ms/batch (trainable parameters)
\end{itemize}

\begin{figure}[t]
\centering
\begin{tikzpicture}[
    node distance=1.2cm and 1.5cm,
    every node/.style={font=\small},
    box/.style={draw, rectangle, rounded corners, minimum width=3cm, minimum height=0.8cm, align=center},
    frozen/.style={box, fill=blue!15, draw=blue!50, very thick, dashed},
    trainable/.style={box, fill=green!15, draw=green!50, thick},
    data/.style={box, fill=gray!10, draw=gray!50},
    loss/.style={box, fill=red!10, draw=red!50},
    arrow/.style={-{Stealth[scale=0.8]}, thick},
    label/.style={font=\footnotesize}
]

% Input trajectory
\node[data] (input) {Input Trajectory\\$\mathbf{r}_{1:t} = (r_1, \ldots, r_t)$\\{\footnotesize Road IDs}};

% Vocabulary mapping
\node[data, below=0.7cm of input, minimum width=4cm] (mapping) {Vocabulary Mapping $\phi$\\{\footnotesize $\mathcal{V} \rightarrow \mathcal{Z}$}};
\draw[arrow] (input) -- (mapping);

% Grid sequence
\node[data, below left=0.7cm and 2cm of mapping] (grid) {Grid Sequence\\$\mathbf{z}_{1:t}$\\{\footnotesize 51,663 cells}};
\draw[arrow] (mapping.south) -- ++(0,-0.3) -| (grid.north);

% Candidate set
\node[data, below right=0.7cm and 2cm of mapping] (candidates) {Candidate Set\\$\mathcal{C}_t \subseteq \mathcal{V}$\\{\footnotesize top-64 roads}};
\draw[arrow] (mapping.south) -- ++(0,-0.3) -| (candidates.north);

% Teacher model (frozen)
\node[frozen, below=1.2cm of grid, minimum width=4.5cm, minimum height=1.2cm] (teacher) {
    \textbf{LM-TAD Teacher}\\
    {\footnotesize Transformer (6 layers)}\\
    {\footnotesize \textit{Frozen weights}}
};
\draw[arrow] (grid) -- (teacher);

% Teacher output
\node[data, below=0.7cm of teacher, minimum width=3.5cm] (teacher_out) {Teacher Probs\\$q(z | \mathbf{z}_{1:t})$};
\draw[arrow] (teacher) -- (teacher_out);

% Student model (trainable)
\node[trainable, below=1.2cm of candidates, minimum width=4.5cm, minimum height=1.2cm] (student) {
    \textbf{HOSER Student}\\
    {\footnotesize Zone GCN + Navigator}\\
    {\footnotesize \textit{Trainable weights}}
};
\draw[arrow] (candidates) -- (student);

% Better arrow from input to student
\coordinate (inputright) at ([xshift=3cm]input.east);
\draw[arrow] (input.east) -- (inputright) |- (student.north east);

% Student output
\node[data, below=0.7cm of student, minimum width=3.5cm] (student_out) {Student Logits\\$\ell_t \in \mathbb{R}^{|\mathcal{C}_t|}$};
\draw[arrow] (student) -- (student_out);

% Temperature scaling and renormalization
\node[data, below right=0.5cm and 0.5cm of teacher_out, minimum width=4cm] (temp) {Temperature Scaling\\$q_t^{(\tau)}, p_t^{(\tau)}$};
\draw[arrow] (teacher_out.south) -- ++(0,-0.2) -| (temp.west);
\draw[arrow] (student_out.south) -- ++(0,-0.2) -| (temp.east);

% KL divergence
\node[loss, below=0.7cm of temp] (kl) {KL Divergence\\$\mathcal{L}_{\text{KL}} = \text{KL}(q_t^{(\tau)} || p_t^{(\tau)})$};
\draw[arrow] (temp) -- (kl);

% Other losses
\node[loss, left=1.2cm of kl] (ce) {Cross-Entropy\\$\mathcal{L}_{\text{CE}}$};
\node[loss, right=1.2cm of kl] (time) {Time MAPE\\$\mathcal{L}_{\text{time}}$};

% Better ground truth connection
\coordinate (leftpoint) at ([xshift=-1.5cm]input.west);
\draw[arrow, dashed] (input.west) -- (leftpoint) -- ([yshift=0cm]leftpoint |- ce.north) -- (ce.north) node[midway, left, label] {\footnotesize Ground truth};

% Student output to time loss
\draw[arrow, dashed] (student_out.east) -- ++(0.5,0) |- (time.east);

% Total loss
\node[loss, below=1cm of kl, minimum width=5cm] (total) {\textbf{Total Loss}\\$\mathcal{L} = \mathcal{L}_{\text{CE}} + \alpha \mathcal{L}_{\text{time}} + \lambda \mathcal{L}_{\text{KL}}$};
\draw[arrow] (ce.south) -- ++(0,-0.3) -| (total.west);
\draw[arrow] (kl) -- (total);
\draw[arrow] (time.south) -- ++(0,-0.3) -| (total.east);

% Gradient flow
\draw[arrow, very thick, red!70] (total.south) -- ++(0,-0.5) node[below, label] {\textbf{Backprop to Student Only}};

% Annotations
\node[above=0.1cm of teacher, font=\footnotesize\bfseries, blue!70] {[Frozen]};
\node[above=0.1cm of student, font=\footnotesize\bfseries, green!70] {[Trainable]};

% Better background box
\begin{scope}[on background layer]
    \coordinate (topleft) at ([xshift=-0.5cm, yshift=0.5cm]input.north west);
    \coordinate (bottomright) at ([xshift=0.5cm, yshift=-1cm]total.south east);
    \node[draw=gray!30, fill=gray!5, fit=(topleft) (bottomright), inner sep=0cm] {};
\end{scope}

\end{tikzpicture}
\caption{Knowledge distillation framework. The frozen LM-TAD teacher (blue, dashed) provides soft targets via temperature-scaled distributions over grid cells. The trainable HOSER student (green) learns from both hard labels (cross-entropy) and soft teacher knowledge (KL divergence). Vocabulary mapping $\phi$ bridges the representation gap between road IDs and grid cells.}
\label{fig:distillation-framework}
\end{figure}

\subsection{Vocabulary Alignment}
\label{sec:method-vocab}

\subsubsection{The Representation Gap}
The teacher and student models operate on fundamentally different spatial representations. LM-TAD discretizes the city into $V = 51{,}663$ uniform grid cells for fine-grained anomaly detection, while HOSER directly predicts over $|\mathcal{V}| = 40{,}060$ road segments for efficient routing. This vocabulary mismatch necessitates a careful alignment strategy.

\subsubsection{Mapping Construction}
We construct a deterministic mapping $\phi: \mathcal{V} \rightarrow \mathcal{Z}$ that assigns each road segment to its corresponding grid cell based on the road's centroid coordinates. Algorithm~\ref{alg:vocab-map} details this preprocessing step.

\begin{algorithm}[t]
\caption{BuildVocabularyMapping}
\label{alg:vocab-map}
\begin{algorithmic}
\Require Road network $\mathcal{V}$ with centroid coordinates, Grid bounds and resolution
\Ensure Mapping $\phi: \mathcal{V} \rightarrow \mathcal{Z}$
\State Initialize $\phi \gets \{\}$
\For{each road $r \in \mathcal{V}$}
    \State $(x_r, y_r) \gets \text{centroid}(r)$
    \State $i \gets \lfloor (x_r - x_{\min}) / \Delta_x \rfloor$ \Comment{Grid row index}
    \State $j \gets \lfloor (y_r - y_{\min}) / \Delta_y \rfloor$ \Comment{Grid column index}
    \State $z \gets i \cdot n_{\text{cols}} + j$ \Comment{Flatten to token ID}
    \State $\phi[r] \gets z$
\EndFor
\State \Return $\phi$
\end{algorithmic}
\end{algorithm}

\begin{remark}[Many-to-One Mapping]
Multiple roads may map to the same grid cell, particularly in dense urban cores. On average, each grid cell corresponds to $|\mathcal{V}|/V \approx 0.78$ roads, though downtown cells may contain 5--10 road segments.
\end{remark}

\subsubsection{Probability Renormalization}
\label{sec:method-renorm}
Given teacher probabilities $q(z | \mathbf{z}_{1:t})$ over all grid cells and student candidates $\mathcal{C}_t$, we extract and renormalize the relevant probabilities:

\begin{align}
\tilde{q}_t(c) &= q(\phi(c) | \mathbf{z}_{1:t}) \quad \forall c \in \mathcal{C}_t \label{eq:extract}\\
q_t^{(\tau)}(c) &= \frac{\exp(\log \tilde{q}_t(c) / \tau)}{\sum_{c' \in \mathcal{C}_t} \exp(\log \tilde{q}_t(c') / \tau)} \label{eq:renorm}
\end{align}

Equation~\eqref{eq:extract} gathers teacher probabilities for candidate roads, while Equation~\eqref{eq:renorm} applies temperature scaling and ensures the distribution sums to 1 over $\mathcal{C}_t$.

\subsection{Knowledge Distillation Framework}
\label{sec:method-distill}

\subsubsection{Temperature Scaling}
\label{sec:method-temp}
Temperature scaling controls the smoothness of probability distributions. For a distribution over logits $\ell$, the temperature-scaled softmax is:

\begin{equation}
p^{(\tau)}_i = \frac{\exp(\ell_i / \tau)}{\sum_j \exp(\ell_j / \tau)}
\label{eq:temp-softmax}
\end{equation}

\begin{definition}[Temperature Effect on Entropy]
As temperature $\tau$ increases:
\begin{itemize}[noitemsep,topsep=0pt]
\item $\tau \rightarrow 1$: Original distribution (sharp)
\item $\tau > 1$: Smoother distribution, higher entropy
\item $\tau \rightarrow \infty$: Uniform distribution
\end{itemize}
\end{definition}

The temperature parameter serves a crucial role in distillation by exposing the relative probabilities of non-maximum choices---what Hinton et al.~\cite{hintonDistillingKnowledgeNeural2015} term ``dark knowledge.''

\subsubsection{Distillation Loss Formulation}
\label{sec:method-kl}
We employ the forward KL divergence to align the student's predictions with the teacher's knowledge. Algorithm~\ref{alg:distill-loss} shows the computation.

\begin{algorithm}[t]
\caption{ComputeDistillationLoss}
\label{alg:distill-loss}
\begin{algorithmic}
\Require Teacher logits $\ell^T_t$, Student logits $\ell^S_t$, Candidates $\mathcal{C}_t$, Temperature $\tau$
\Ensure Distillation loss $\mathcal{L}_{\text{KL}}$
\State $q^{(\tau)}_t \gets \text{Softmax}(\ell^T_t / \tau)$ \Comment{Teacher distribution}
\State $p^{(\tau)}_t \gets \text{Softmax}(\ell^S_t / \tau)$ \Comment{Student distribution}
\State $\mathcal{L}_{\text{KL}} \gets 0$
\For{each candidate $c \in \mathcal{C}_t$}
    \If{$q^{(\tau)}_t(c) > 0$} \Comment{Avoid $\log(0)$}
        \State $\mathcal{L}_{\text{KL}} \gets \mathcal{L}_{\text{KL}} + q^{(\tau)}_t(c) \cdot [\log q^{(\tau)}_t(c) - \log p^{(\tau)}_t(c)]$
    \EndIf
\EndFor
\State $\mathcal{L}_{\text{KL}} \gets \tau^2 \cdot \mathcal{L}_{\text{KL}}$ \Comment{Gradient correction}
\State \Return $\mathcal{L}_{\text{KL}}$
\end{algorithmic}
\end{algorithm}

\begin{remark}[Gradient Scaling]
The $\tau^2$ factor in line 9 ensures that gradients remain well-scaled as $\tau$ increases, preventing gradient vanishing during backpropagation~\cite{hintonDistillingKnowledgeNeural2015}.
\end{remark}

\subsubsection{Combined Training Objective}
\label{sec:method-combined}
The total loss combines three components, each serving a distinct purpose:

\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{CE}} + \alpha \cdot \mathcal{L}_{\text{time}} + \lambda \cdot \mathcal{L}_{\text{KL}}
\label{eq:total-loss}
\end{equation}

where:
\begin{itemize}[noitemsep,topsep=0pt]
\item $\mathcal{L}_{\text{CE}}$: Cross-entropy loss against ground-truth road (hard target)
\item $\mathcal{L}_{\text{time}}$: Mean Absolute Percentage Error for travel time prediction
\item $\mathcal{L}_{\text{KL}}$: Knowledge distillation loss (soft target)
\end{itemize}

The hyperparameters $\alpha = 0.1$ (fixed) and $\lambda \in [0.001, 0.1]$ (tuned) balance the loss components.

\subsection{Training Pipeline}
\label{sec:method-training}

\subsubsection{Main Training Loop}
Algorithm~\ref{alg:distill-train} presents the complete distillation training procedure, integrating teacher inference, loss computation, and optimization.

\begin{algorithm}[t]
\caption{DistillationTraining}
\label{alg:distill-train}
\begin{algorithmic}
\Require Dataset $\mathcal{D}$, Teacher $f_T$, Student $f_S$, Hyperparameters $\{\lambda, \tau, w, \eta\}$
\Ensure Trained student parameters $\theta_S$
\State Initialize student parameters $\theta_S$
\State Freeze teacher parameters $\theta_T$ \Comment{No gradients}
\For{epoch $= 1$ to $E$}
    \For{batch $(\mathbf{r}, \mathbf{y}, \mathbf{t}) \in \mathcal{D}$} \Comment{Roads, labels, times}
        \State $\mathbf{z} \gets \phi(\mathbf{r})$ \Comment{Map roads to grid cells}
        \State \textbf{// Teacher inference (no gradient)}
        \State $\mathbf{q} \gets f_T(\mathbf{z}_{t-w:t})$ with \texttt{torch.no\_grad()}
        \State Extract $q_t^{(\tau)}$ for candidates $\mathcal{C}_t$ \Comment{Eq.~\eqref{eq:renorm}}
        \State \textbf{// Student forward pass}
        \State $\mathbf{\ell} \gets f_S(\mathbf{r}, \mathcal{C})$ \Comment{Student logits}
        \State $p_t^{(\tau)} \gets \text{Softmax}(\mathbf{\ell} / \tau)$
        \State \textbf{// Compute losses}
        \State $\mathcal{L}_{\text{CE}} \gets \text{CrossEntropy}(\mathbf{\ell}, \mathbf{y})$
        \State $\mathcal{L}_{\text{time}} \gets \text{MAPE}(\hat{\mathbf{t}}, \mathbf{t})$
        \State $\mathcal{L}_{\text{KL}} \gets \text{ComputeDistillationLoss}(q_t^{(\tau)}, p_t^{(\tau)}, \tau)$
        \State $\mathcal{L} \gets \mathcal{L}_{\text{CE}} + \alpha \cdot \mathcal{L}_{\text{time}} + \lambda \cdot \mathcal{L}_{\text{KL}}$
        \State \textbf{// Optimization step}
        \State $\nabla_{\theta_S} \mathcal{L} \gets \text{Backward}(\mathcal{L})$
        \State $\theta_S \gets \text{AdamW}(\theta_S, \nabla_{\theta_S} \mathcal{L}, \eta)$
    \EndFor
    \State Validate and save best checkpoint
\EndFor
\State \Return $\theta_S$
\end{algorithmic}
\end{algorithm}

\begin{remark}[Computational Complexity]
Per-iteration complexity: $\mathcal{O}(B \cdot t \cdot (V + k^2))$ where $B$ is batch size, $t$ is trajectory length, $V$ is vocabulary size, and $k$ is candidate set size. The teacher inference dominates at $\mathcal{O}(B \cdot t \cdot V)$. Practical optimizations and hyperparameter tuning are detailed in \autoref{sec:implementation}.
\end{remark}

\subsection{Inference and Evaluation}
\label{sec:method-inference}

\subsubsection{Trajectory Generation}
At inference time, we use only the trained student model to generate complete trajectories. Algorithm~\ref{alg:beam-search} details our beam search procedure.

\begin{algorithm}[t]
\caption{BeamSearchGeneration}
\label{alg:beam-search}
\begin{algorithmic}
\Require Origin $r_o$, Destination $r_d$, Student model $f_S$, Beam width $b$
\Ensure Generated trajectory $\hat{\mathbf{r}}$
\State Initialize beams $\mathcal{B} \gets \{(r_o, 0.0)\}$ \Comment{(path, log-prob)}
\State $t \gets 0$
\While{$t < T_{\max}$ and no beam reached $r_d$}
    \State $\mathcal{B}_{\text{new}} \gets \{\}$
    \For{each $(path, score) \in \mathcal{B}$}
        \State $r_{\text{curr}} \gets \text{last}(path)$
        \State $\mathcal{C} \gets \text{GetCandidates}(r_{\text{curr}}, r_d)$ \Comment{Spatial pruning}
        \State $\mathbf{\ell} \gets f_S(path, \mathcal{C})$ \Comment{Student inference only}
        \State $\mathbf{p} \gets \text{Softmax}(\mathbf{\ell})$
        \For{each $c \in \text{top-}k(\mathbf{p}, b)$}
            \State $path' \gets path + [c]$
            \State $score' \gets score + \log p(c)$
            \State $\mathcal{B}_{\text{new}} \gets \mathcal{B}_{\text{new}} \cup \{(path', score')\}$
        \EndFor
    \EndFor
    \State $\mathcal{B} \gets \text{top-}b(\mathcal{B}_{\text{new}})$ by score
    \State $t \gets t + 1$
\EndWhile
\State \Return best complete path from $\mathcal{B}$
\end{algorithmic}
\end{algorithm}

\begin{remark}[Inference Speed]
With beam width $b=4$, the student generates trajectories at $\sim$77 trajectories/second on a single GPU, enabling real-time deployment for urban-scale applications.
\end{remark}

\subsubsection{Evaluation Protocol}
\label{sec:method-eval-metrics}
We evaluate generated trajectories using two categories of metrics:

\textbf{Global Distribution Metrics:}
\begin{itemize}[noitemsep,topsep=0pt]
\item \textbf{Jensen-Shannon Divergence (JSD)}: Measures similarity of aggregate statistics
  \begin{equation}
  \text{JSD}(P||Q) = \frac{1}{2}D_{KL}(P||M) + \frac{1}{2}D_{KL}(Q||M)
  \end{equation}
  where $M = \frac{1}{2}(P + Q)$ and evaluated for distance, duration, and radius distributions
\end{itemize}

\textbf{Local Trajectory Metrics:}
\begin{itemize}[noitemsep,topsep=0pt]
\item \textbf{Hausdorff Distance}: Maximum deviation between trajectories
\item \textbf{Dynamic Time Warping (DTW)}: Temporal alignment distance
\item \textbf{Edit Distance on Real sequence (EDR)}: Discrete sequence similarity
\item \textbf{Path Completion Rate}: Percentage reaching destination
\end{itemize}

All metrics are computed separately on training OD pairs (memorization) and held-out test OD pairs (generalization) to assess overfitting.

\bigskip

This completes the theoretical framework for knowledge distillation. Practical implementation details, including dataset preprocessing, hyperparameter optimization, training optimizations, and computational infrastructure, are described in \autoref{sec:implementation}.