\section{Methodology}
\label{sec:methodology}

This section presents our knowledge distillation framework for transferring spatial reasoning from a trajectory anomaly detector to a fast route predictor. Figure~\ref{fig:distillation-framework} illustrates the complete pipeline, showing how the frozen teacher and trainable student interact through vocabulary mapping and temperature-scaled distributions. We detail the vocabulary alignment mechanism (\autoref{sec:method-vocab}), the temperature-scaled distillation objective (\autoref{sec:method-kl}), the optimized training pipeline (\autoref{sec:method-training}), and the inference procedure (\autoref{sec:method-inference}).

\subsection{Preliminaries}
\label{sec:method-prelim}

\subsubsection{Notation}
Table~\ref{tab:notation} summarizes the mathematical notation used throughout this section.

\begin{table}[h]
    \centering
    \caption{Mathematical notation}
    \label{tab:notation}
    \begin{tabular}{ll}
        \toprule
        \textbf{Symbol}                                                                    & \textbf{Description}                             \\
        \midrule
        $\mathcal{G} = (\mathcal{V}, \mathcal{E})$                                         & Road network graph with $|\mathcal{V}|$ segments \\
        $\mathbf{r}_{1:t} = (r_1, \ldots, r_t)$                                            & Partial trajectory of $t$ road segments          \\
        $\mathcal{C}_t \subseteq \mathcal{V}$                                              & Candidate set at timestep $t$ (top-$k$ roads)    \\
        $\mathcal{Z} = \{1, \ldots, V\}$                                                   & Grid cell vocabulary, $V = 51{,}663$             \\
        \midrule
        $\mathcal{L}_\phi : \mathcal{Z}^w \to \Delta^{|\mathcal{Z}|}$                      & Teacher model (frozen)                           \\
        $\mathcal{H}_\theta : \mathcal{V}^t \times \mathcal{V} \to \Delta^{|\mathcal{C}|}$ & Student model (trainable)                        \\
        $\psi : \mathcal{V} \to \mathcal{Z}$                                               & Cross-vocabulary mapping                         \\
        $q^{(\tau)}, p^{(\tau)}$                                                           & Temperature-scaled distributions                 \\
        $\tau$                                                                             & Temperature parameter                            \\
        $\lambda$                                                                          & Distillation loss weight                         \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Problem Definition}
\label{sec:method-problem}

\begin{definition}[Trajectory]
    \label{def:trajectory}
    A \emph{trajectory} $T$ is a sequence of road segments:
    \[
        T = \{r_1, r_2, \ldots, r_n\} \quad \text{where } r_i \in \mathcal{V}
    \]
    representing a path through the road network $\mathcal{G} = (\mathcal{V}, \mathcal{E})$.
\end{definition}

\begin{definition}[Timestamped Trajectory]
    \label{def:trajectory-timestamped}
    A \emph{timestamped trajectory} $T$ includes arrival times at each road segment:
    \[
        T = \{(r_1, t_1), (r_2, t_2), \ldots, (r_n, t_n)\} \quad \text{where } r_i \in \mathcal{V}, t_i \in \mathbb{R}_+
    \]
    with timestamps satisfying $t_1 < t_2 < \cdots < t_n$.
\end{definition}

\begin{definition}[Cross-Vocabulary Mapping]
    \label{def:vocab-mapping}
    The mapping $\psi : \mathcal{V} \to \mathcal{Z}$ assigns each road segment to its containing grid cell:
    \begin{equation}
        \psi(v) = \left\lfloor \frac{x_v - x_{\min}}{\Delta_x} \right\rfloor \cdot n_{\text{cols}} +
        \left\lfloor \frac{y_v - y_{\min}}{\Delta_y} \right\rfloor
    \end{equation}
    where $(x_v, y_v) = \text{centroid}(v)$ and $\Delta_x, \Delta_y$ are grid resolutions.
\end{definition}

\begin{definition}[Cross-Task Knowledge Distillation]
    \label{def:distillation-problem}
    Given frozen teacher $\mathcal{L}_\phi$ trained for anomaly detection and trajectory dataset $\mathcal{D} = \{(\mathbf{r}_i, y_i)\}_{i=1}^N$, learn student parameters:
    \begin{equation}
        \theta^* = \arg\min_{\theta \in \Theta} \mathbb{E}_{(\mathbf{r}, y) \sim \mathcal{D}}
        \left[ \mathcal{L}_{\text{CE}}(\mathcal{H}_\theta(\mathbf{r}), y) +
        \lambda \mathcal{L}_{\text{KL}}^{(\tau)}(\mathcal{L}_\phi \circ \psi(\mathbf{r}), \mathcal{H}_\theta(\mathbf{r})) \right]
    \end{equation}
\end{definition}

\subsubsection{Model Specifications}
\label{sec:method-models}

We transfer knowledge from the pre-trained LM-TAD teacher $\mathcal{L}_\phi$~\cite{mbuyaTrajectoryAnomalyDetection2024} to the HOSER student $\mathcal{H}_\theta$~\cite{caoHolisticSemanticRepresentation2025}. The teacher operates on grid cell sequences with vocabulary size $|\mathcal{Z}| = 51{,}663$, while the student predicts over $|\mathcal{V}| = 40{,}060$ road segments.

\begin{figure}[t]
    \centering
    \begin{tikzpicture}[
        node distance=1.2cm and 1.5cm,
        every node/.style={font=\small},
        box/.style={draw, rectangle, rounded corners, minimum width=3cm, minimum height=0.8cm, align=center},
        frozen/.style={box, fill=blue!15, draw=blue!50, very thick, dashed},
        trainable/.style={box, fill=green!15, draw=green!50, thick},
        data/.style={box, fill=gray!10, draw=gray!50},
        loss/.style={box, fill=red!10, draw=red!50},
        arrow/.style={-{Stealth[scale=0.8]}, thick},
        label/.style={font=\footnotesize}
        ]

        % Input trajectory
        \node[data] (input) {Input Trajectory\\$\mathbf{r}_{1:t} = (r_1, \ldots, r_t)$\\{\footnotesize Road IDs}};

            % Vocabulary mapping
            \node[data, below=0.7cm of input, minimum width=4cm] (mapping) {Vocabulary Mapping $\psi$\\{\footnotesize $\mathcal{V} \rightarrow \mathcal{Z}$}};
            \draw[arrow] (input) -- (mapping);

            % Grid sequence
            \node[data, below left=0.7cm and 2cm of mapping] (grid) {Grid Sequence\\$\mathbf{z}_{1:t}$\\{\footnotesize 51,663 cells}};
        \draw[arrow] (mapping.south) -- ++(0,-0.3) -| (grid.north);

        % Candidate set
        \node[data, below right=0.7cm and 2cm of mapping] (candidates) {Candidate Set\\$\mathcal{C}_t \subseteq \mathcal{V}$\\{\footnotesize top-64 roads}};
            \draw[arrow] (mapping.south) -- ++(0,-0.3) -| (candidates.north);

            % Teacher model (frozen)
            \node[frozen, below=1.2cm of grid, minimum width=4.5cm, minimum height=1.2cm] (teacher) {
            \textbf{LM-TAD Teacher}\\
            {\footnotesize Transformer (6 layers)}\\
            {\footnotesize \textit{Frozen weights}}
            };
            \draw[arrow] (grid) -- (teacher);

            % Teacher output
            \node[data, below=0.7cm of teacher, minimum width=3.5cm] (teacher_out) {Teacher Probs\\$q(z | \mathbf{z}_{1:t})$};
            \draw[arrow] (teacher) -- (teacher_out);

            % Student model (trainable)
            \node[trainable, below=1.2cm of candidates, minimum width=4.5cm, minimum height=1.2cm] (student) {
            \textbf{HOSER Student}\\
            {\footnotesize Zone GCN + Navigator}\\
            {\footnotesize \textit{Trainable weights}}
            };
            \draw[arrow] (candidates) -- (student);

            % Better arrow from input to student
            \coordinate (inputright) at ([xshift=3cm]input.east);
            \draw[arrow] (input.east) -- (inputright) |- (student.north east);

            % Student output
            \node[data, below=0.7cm of student, minimum width=3.5cm] (student_out) {Student Logits\\$\ell_t \in \mathbb{R}^{|\mathcal{C}_t|}$};
        \draw[arrow] (student) -- (student_out);

        % Temperature scaling and renormalization
        \node[data, below right=0.5cm and 0.5cm of teacher_out, minimum width=4cm] (temp) {Temperature Scaling\\$q_t^{(\tau)}, p_t^{(\tau)}$};
            \draw[arrow] (teacher_out.south) -- ++(0,-0.2) -| (temp.west);
            \draw[arrow] (student_out.south) -- ++(0,-0.2) -| (temp.east);

            % KL divergence
            \node[loss, below=0.7cm of temp] (kl) {KL Divergence\\$\mathcal{L}_{\text{KL}} = \text{KL}(q_t^{(\tau)} || p_t^{(\tau)})$};
        \draw[arrow] (temp) -- (kl);

        % Other losses
        \node[loss, left=1.2cm of kl] (ce) {Cross-Entropy\\$\mathcal{L}_{\text{CE}}$};
            \node[loss, right=1.2cm of kl] (time) {Time MAPE\\$\mathcal{L}_{\text{time}}$};

        % Better ground truth connection
        \coordinate (leftpoint) at ([xshift=-1.5cm]input.west);
        \draw[arrow, dashed] (input.west) -- (leftpoint) -- ([yshift=0cm]leftpoint |- ce.north) -- (ce.north) node[midway, left, label] {\footnotesize Ground truth};

        % Student output to time loss
        \draw[arrow, dashed] (student_out.east) -- ++(0.5,0) |- (time.east);

        % Total loss
        \node[loss, below=1cm of kl, minimum width=5cm] (total) {\textbf{Total Loss}\\$\mathcal{L} = \mathcal{L}_{\text{CE}} + \alpha \mathcal{L}_{\text{time}} + \lambda \mathcal{L}_{\text{KL}}$};
        \draw[arrow] (ce.south) -- ++(0,-0.3) -| (total.west);
        \draw[arrow] (kl) -- (total);
        \draw[arrow] (time.south) -- ++(0,-0.3) -| (total.east);

        % Gradient flow
        \draw[arrow, very thick, red!70] (total.south) -- ++(0,-0.5) node[below, label] {\textbf{Backprop to Student Only}};

        % Annotations
        \node[above=0.1cm of teacher, font=\footnotesize\bfseries, blue!70] {[Frozen]};
        \node[above=0.1cm of student, font=\footnotesize\bfseries, green!70] {[Trainable]};

        % Better background box
        \begin{scope}[on background layer]
            \coordinate (topleft) at ([xshift=-0.5cm, yshift=0.5cm]input.north west);
            \coordinate (bottomright) at ([xshift=0.5cm, yshift=-1cm]total.south east);
            \node[draw=gray!30, fill=gray!5, fit=(topleft) (bottomright), inner sep=0cm] {};
        \end{scope}

    \end{tikzpicture}
    \caption{Knowledge distillation framework. The frozen teacher $\mathcal{L}_\phi$ provides soft targets via temperature-scaled distributions. The trainable student $\mathcal{H}_\theta$ learns from both hard labels (cross-entropy) and soft teacher knowledge (KL divergence). The mapping $\psi$ bridges road segments to grid cells.}
    \label{fig:distillation-framework}
\end{figure}

\subsection{Vocabulary Alignment}
\label{sec:method-vocab}

The cross-vocabulary mapping $\psi$ (\autoref{def:vocab-mapping}) enables knowledge transfer between the teacher's grid-based representation and the student's road-based representation. The mapping assigns each road segment to its containing grid cell based on centroid coordinates (detailed algorithm in \hyperref[app:vocab-mapping-alg]{Appendix~\ref*{app:vocab-mapping-alg}}).

\begin{remark}[Many-to-One Mapping]
    Multiple roads may map to the same grid cell, particularly in dense urban cores. On average, each grid cell corresponds to $|\mathcal{V}|/V \approx 0.78$ roads, though downtown cells may contain 5--10 road segments.
\end{remark}

\subsubsection{Probability Renormalization}
\label{sec:method-renorm}

Since the teacher produces distributions over all grid cells $\mathcal{Z}$ while the student operates over candidate roads $\mathcal{C}_t$, we extract and renormalize:
\begin{equation}
    \tilde{q}_t(c) = q(\psi(c) \mid \mathbf{z}_{1:t}) \quad \forall c \in \mathcal{C}_t
    \label{eq:extract}
\end{equation}
\begin{equation}
    q_t^{(\tau)}(c) = \frac{\exp(\log \tilde{q}_t(c) / \tau)}{\sum_{c' \in \mathcal{C}_t} \exp(\log \tilde{q}_t(c') / \tau)}
    \label{eq:renorm}
\end{equation}

\subsection{Knowledge Distillation Framework}
\label{sec:method-kl}

\subsubsection{Temperature Scaling}

\begin{theorem}[Temperature-Scaled Knowledge Transfer]
    \label{thm:temp-scaling}
    Given teacher logits $\boldsymbol{\ell}^{\mathcal{L}} \in \mathbb{R}^{|\mathcal{C}|}$ and student logits $\boldsymbol{\ell}^{\mathcal{H}} \in \mathbb{R}^{|\mathcal{C}|}$, the distillation loss is:
    \begin{equation}
        \mathcal{L}_{\text{KL}}^{(\tau)} = \tau^2 \sum_{i \in \mathcal{C}_t} q_i^{(\tau)} \log \frac{q_i^{(\tau)}}{p_i^{(\tau)}}
        \label{eq:distill-loss}
    \end{equation}
    where:
    \begin{align}
        q_i^{(\tau)} & = \frac{\exp(\ell_i^{\mathcal{L}} / \tau)}{\sum_{j \in \mathcal{C}_t} \exp(\ell_j^{\mathcal{L}} / \tau)} \label{eq:teacher-dist} \\
        p_i^{(\tau)} & = \frac{\exp(\ell_i^{\mathcal{H}} / \tau)}{\sum_{j \in \mathcal{C}_t} \exp(\ell_j^{\mathcal{H}} / \tau)} \label{eq:student-dist}
    \end{align}
    The $\tau^2$ scaling factor preserves gradient magnitudes as $\tau$ increases.
\end{theorem}

\begin{proof}
    The gradient with respect to student logit $\ell_i^{\mathcal{H}}$ is:
    \begin{equation}
        \frac{\partial \mathcal{L}_{\text{KL}}}{\partial \ell_i^{\mathcal{H}}} = \frac{1}{\tau}(p_i^{(\tau)} - q_i^{(\tau)})
    \end{equation}
    which scales as $\mathcal{O}(1/\tau)$. Multiplying by $\tau^2$ ensures $\mathcal{O}(\tau)$ scaling, preventing gradient vanishing in the high-temperature regime where distillation is most effective~\cite{hintonDistillingKnowledgeNeural2015}.
\end{proof}

\subsubsection{Combined Training Objective}

The total loss combines supervised learning with knowledge distillation:
\begin{equation}
    \mathcal{L}_{\text{total}} = \underbrace{\mathcal{L}_{\text{CE}}(p, y)}_{\text{hard targets}} +
    \alpha \underbrace{\mathcal{L}_{\text{time}}(\hat{t}, t)}_{\text{auxiliary}} +
    \lambda \underbrace{\mathcal{L}_{\text{KL}}^{(\tau)}(q^{(\tau)}, p^{(\tau)})}_{\text{soft targets}}
    \label{eq:total-loss}
\end{equation}
where $\alpha = 0.1$ (fixed) and $\lambda \in [0.001, 0.1]$ (tuned).

\subsection{Training Pipeline}
\label{sec:method-training}

\begin{algorithm}[t]
    \caption{Knowledge Distillation Training}
    \label{alg:distillation}
    \begin{algorithmic}[1]
        \Require Dataset $\mathcal{D}$, Teacher $\mathcal{L}_\phi$, Student $\mathcal{H}_\theta$, hyperparameters $\{\lambda, \tau, w, \eta\}$
        \Ensure Trained parameters $\theta^*$
        \State Initialize $\theta \sim \mathcal{N}(0, 0.02)$
        \For{epoch $= 1$ to $E$}
        \For{$(\mathbf{r}, y) \in \mathcal{D}$}
        \State $\mathbf{z} \gets \psi(\mathbf{r})$ \Comment{Map roads to grid cells}
        \State $\mathbf{q} \gets \mathcal{L}_\phi(\mathbf{z}_{t-w:t})$ \Comment{Teacher inference (no gradient)}
        \State $q^{(\tau)} \gets \text{Renormalize}(\mathbf{q}, \mathcal{C}_t, \tau)$ \Comment{Eq.~\eqref{eq:renorm}}
        \State $\boldsymbol{\ell} \gets \mathcal{H}_\theta(\mathbf{r}, \mathcal{C}_t)$ \Comment{Student logits}
        \State $p^{(\tau)} \gets \text{Softmax}(\boldsymbol{\ell} / \tau)$
        \State $\mathcal{L} \gets \mathcal{L}_{\text{CE}}(\boldsymbol{\ell}, y) + \lambda \mathcal{L}_{\text{KL}}^{(\tau)}(q^{(\tau)}, p^{(\tau)})$
        \State $\theta \gets \theta - \eta \nabla_\theta \mathcal{L}$
        \EndFor
        \EndFor
        \State \Return $\theta^*$
    \end{algorithmic}
\end{algorithm}

The teacher parameters $\phi$ remain frozen throughout training. Per-iteration complexity is $\mathcal{O}(B \cdot t \cdot (V + k^2))$ where teacher inference dominates at $\mathcal{O}(B \cdot t \cdot V)$. Implementation details are provided in \autoref{sec:implementation}.

\subsection{Inference and Evaluation}
\label{sec:method-inference}

At inference time, we employ only the trained student $\mathcal{H}_{\theta^*}$ with standard beam search (width $b=4$) to generate trajectories. Given origin-destination pair $(r_o, r_d)$, the student produces complete trajectories at $\sim$77 trajectories/second.

We evaluate using global distribution metrics (Jensen-Shannon Divergence) and local trajectory metrics (Hausdorff, DTW, EDR) computed separately on training and test OD pairs. Full evaluation details are provided in \autoref{sec:eval-metrics}.