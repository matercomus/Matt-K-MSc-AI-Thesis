\section{Methodology}
\label{sec:methodology}

This section presents our knowledge distillation framework for transferring spatial reasoning from a trajectory anomaly detector to a fast route predictor. Figure~\ref{fig:distillation-framework} illustrates the complete pipeline, showing how the frozen teacher and trainable student interact through vocabulary mapping and temperature-scaled distributions. We detail the vocabulary alignment mechanism (\autoref{sec:method-vocab}), the temperature-scaled distillation objective (\autoref{sec:method-kl}), the optimized training pipeline (\autoref{sec:method-training}), and the inference procedure (\autoref{sec:method-inference}).

\subsection{Preliminaries}
\label{sec:method-prelim}

\subsubsection{Notation}
Table~\ref{tab:notation} summarizes the mathematical notation used throughout this section.

\begin{table}[h]
    \centering
    \caption{Mathematical notation}
    \label{tab:notation}
    \begin{tabular}{ll}
        \toprule
        \textbf{Symbol}                                                                    & \textbf{Description}                             \\
        \midrule
        $\mathcal{G} = (\mathcal{V}, \mathcal{E})$                                         & Road network graph with $|\mathcal{V}|$ segments \\
        $\mathbf{r}_{1:t} = (r_1, \ldots, r_t)$                                            & Partial trajectory of $t$ road segments          \\
        $\mathcal{C}_t \subseteq \mathcal{V}$                                              & Candidate set at timestep $t$ (top-$k$ roads)    \\
        $\mathcal{Z} = \{1, \ldots, V\}$                                                   & Grid cell vocabulary             \\
        \midrule
        $\mathcal{L}_\phi : \mathcal{Z}^w \to \Delta^{|\mathcal{Z}|}$                      & Teacher model (frozen)                           \\
        $\mathcal{H}_\theta : \mathcal{V}^t \times \mathcal{V} \to \Delta^{|\mathcal{C}|}$ & Student model (trainable)                        \\
        $\psi : \mathcal{V} \to \mathcal{Z}$                                               & Cross-vocabulary mapping                         \\
        $q^{(\tau)}, p^{(\tau)}$                                                           & Temperature-scaled distributions                 \\
        $\tau$                                                                             & Temperature parameter                            \\
        $\lambda$                                                                          & Distillation loss weight                         \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Problem Definition}
\label{sec:method-problem}

\begin{definition}[Trajectory]
    \label{def:trajectory}
    A \emph{trajectory} $T$ is a sequence of road segments:
    \[
        T = \{r_1, r_2, \ldots, r_n\} \quad \text{where } r_i \in \mathcal{V}
    \]
    representing a path through the road network $\mathcal{G} = (\mathcal{V}, \mathcal{E})$.
\end{definition}

\begin{definition}[Timestamped Trajectory]
    \label{def:trajectory-timestamped}
    A \emph{timestamped trajectory} $T$ includes arrival times at each road segment:
    \[
        T = \{(r_1, t_1), (r_2, t_2), \ldots, (r_n, t_n)\} \quad \text{where } r_i \in \mathcal{V}, t_i \in \mathbb{R}_+
    \]
    with timestamps satisfying $t_1 < t_2 < \cdots < t_n$.
\end{definition}

\begin{definition}[Cross-Vocabulary Mapping]
    \label{def:vocab-mapping}
    The mapping $\psi : \mathcal{V} \to \mathcal{Z}$ assigns each road segment to its containing grid cell:
    \begin{equation}
        \psi(v) = \left\lfloor \frac{x_v - x_{\min}}{\Delta_x} \right\rfloor \cdot n_{\text{cols}} +
        \left\lfloor \frac{y_v - y_{\min}}{\Delta_y} \right\rfloor
    \end{equation}
    where $(x_v, y_v) = \text{centroid}(v)$ and $\Delta_x, \Delta_y$ are grid resolutions.
\end{definition}

\begin{definition}[Cross-Task Knowledge Distillation]
    \label{def:distillation-problem}
    Given frozen teacher $\mathcal{L}_\phi$ trained for anomaly detection and trajectory dataset $\mathcal{D} = \{(\mathbf{r}_i, y_i)\}_{i=1}^N$, learn student parameters:
    \begin{equation}
        \theta^* = \arg\min_{\theta \in \Theta} \mathbb{E}_{(\mathbf{r}, y) \sim \mathcal{D}}
        \left[ \mathcal{L}_{\text{CE}}(\mathcal{H}_\theta(\mathbf{r}), y) +
        \lambda \mathcal{L}_{\text{KL}}^{(\tau)}(\mathcal{L}_\phi \circ \psi(\mathbf{r}), \mathcal{H}_\theta(\mathbf{r})) \right]
    \end{equation}
\end{definition}

\subsubsection{Model Specifications}
\label{sec:method-models}

We transfer knowledge from the pre-trained LM-TAD teacher $\mathcal{L}_\phi$~\cite{mbuyaTrajectoryAnomalyDetection2024} to the HOSER student $\mathcal{H}_\theta$~\cite{caoHolisticSemanticRepresentation2025}. The teacher operates on grid cell sequences with vocabulary size $|\mathcal{Z}| = V$, while the student predicts over $|\mathcal{V}|$ road segments.

\input{sections/figures/fig-distillation-framework}

\subsection{Vocabulary Alignment}
\label{sec:method-vocab}

The cross-vocabulary mapping $\psi$ (\autoref{def:vocab-mapping}) enables knowledge transfer between the teacher's grid-based representation and the student's road-based representation. The mapping assigns each road segment to its containing grid cell based on centroid coordinates (detailed algorithm in \hyperref[app:vocab-mapping-alg]{Appendix~\ref*{app:vocab-mapping-alg}}).

\begin{remark}[Many-to-One Mapping]
    Multiple roads may correspond to a single grid cell, with higher density in urban cores.
\end{remark}

\begin{remark}[Output Space Alignment]
    The cross-vocabulary mapping introduces distributional differences between the teacher's grid-cell representation and the student's road-segment representation. Recent work on cross-vocabulary KD~\cite{zhangDualSpaceFrameworkGeneral2025} demonstrates that bridging distributions from different output spaces can limit teacher-student similarity. Our approach employs a fixed deterministic mapping $\psi$ based on spatial centroids, prioritizing computational simplicity and interpretability over alignment complexity.
\end{remark}

\subsubsection{Probability Renormalization}
\label{sec:method-renorm}

Since the teacher produces distributions over all grid cells $\mathcal{Z}$ while the student operates over candidate roads $\mathcal{C}_t$, we extract and renormalize:
\begin{equation}
    \tilde{q}_t(c) = q(\psi(c) \mid \mathbf{z}_{1:t}) \quad \forall c \in \mathcal{C}_t
    \label{eq:extract}
\end{equation}
\begin{equation}
    q_t^{(\tau)}(c) = \frac{\exp(\log \tilde{q}_t(c) / \tau)}{\sum_{c' \in \mathcal{C}_t} \exp(\log \tilde{q}_t(c') / \tau)}
    \label{eq:renorm}
\end{equation}

\subsection{Knowledge Distillation Framework}
\label{sec:method-kl}

\subsubsection{Temperature Scaling}

\begin{theorem}[Temperature-Scaled Knowledge Transfer]
    \label{thm:temp-scaling}
    Given teacher logits $\boldsymbol{\ell}^{\mathcal{L}} \in \mathbb{R}^{|\mathcal{C}|}$ and student logits $\boldsymbol{\ell}^{\mathcal{H}} \in \mathbb{R}^{|\mathcal{C}|}$, the distillation loss is:
    \begin{equation}
        \mathcal{L}_{\text{KL}}^{(\tau)} = \tau^2 \sum_{i \in \mathcal{C}_t} q_i^{(\tau)} \log \frac{q_i^{(\tau)}}{p_i^{(\tau)}}
        \label{eq:distill-loss}
    \end{equation}
    where:
    \begin{align}
        q_i^{(\tau)} & = \frac{\exp(\ell_i^{\mathcal{L}} / \tau)}{\sum_{j \in \mathcal{C}_t} \exp(\ell_j^{\mathcal{L}} / \tau)} \label{eq:teacher-dist} \\
        p_i^{(\tau)} & = \frac{\exp(\ell_i^{\mathcal{H}} / \tau)}{\sum_{j \in \mathcal{C}_t} \exp(\ell_j^{\mathcal{H}} / \tau)} \label{eq:student-dist}
    \end{align}
    The $\tau^2$ scaling factor preserves gradient magnitudes as $\tau$ increases.
\end{theorem}

\begin{proof}
    The gradient with respect to student logit $\ell_i^{\mathcal{H}}$ is:
    \begin{equation}
        \frac{\partial \mathcal{L}_{\text{KL}}}{\partial \ell_i^{\mathcal{H}}} = \frac{1}{\tau}(p_i^{(\tau)} - q_i^{(\tau)})
    \end{equation}
    which scales as $\mathcal{O}(1/\tau)$. Multiplying by $\tau^2$ ensures $\mathcal{O}(\tau)$ scaling, preventing gradient vanishing in the high-temperature regime where distillation is most effective~\cite{hintonDistillingKnowledgeNeural2015}.
\end{proof}

The reverse KL divergence has several desirable properties for knowledge distillation. It exhibits mode-seeking behavior, encouraging the student to learn specific behaviors from the teacher rather than spreading probability across multiple suboptimal options. Additionally, it reduces exposure bias by training the student on its own predicted distributions~\cite{OnPolicyDistillation}. Recent work demonstrates that this loss function enables dense, per-token supervision that is significantly more compute-efficient than sparse reward signals~\cite{OnPolicyDistillation}.

The role of temperature in distillation has been extensively studied. Higher temperatures smooth the output distribution, enabling transfer of relational information between classes~\cite{hintonDistillingKnowledgeNeural2015}. Recent work on LLM distillation emphasizes that temperature helps preserve diversity in the target distribution, preventing the narrowing effect that can occur with aggressive supervision~\cite{singhORPODistillMixedPolicyPreference2025}. The optimal temperature is task- and dataset-dependent, requiring empirical tuning for each application.

\paragraph{Hyperparameter Optimization}
We employ Optuna with the CMA-ES sampler to identify optimal distillation hyperparameters for each dataset. The search space spans $\lambda \in [0.001, 0.1]$ (log scale), $\tau \in [1.0, 5.0]$ (linear scale), and window size $w \in [2, 8]$ (integer). For the Beijing dataset, a 12-trial optimization identified optimal values:
\begin{itemize}[leftmargin=*,noitemsep]
    \item $\lambda = 0.0014$ (KL divergence weight)
    \item $\tau = 4.37$ (temperature)
    \item $w = 7$ (teacher context window, steps)
\end{itemize}
These hyperparameters achieve 57.24\% validation accuracy (marginal +0.01\% over vanilla's 57.23\%), yet produce dramatic generation quality improvements (85--89\% OD match rate vs vanilla's 12\%). This disconnect between validation and generation metrics indicates that next-step prediction accuracy is a poor proxy for long-horizon trajectory realism.

\textbf{Dataset-specific tuning is mandatory}---Porto optimal hyperparameters diverge significantly from Beijing: $\lambda = 0.00598$ (4.3$\times$ higher), $\tau = 2.515$ (42\% lower), $w = 4$ (43\% shorter). Cross-dataset hyperparameter transfer fails, requiring per-dataset HPO study.

\subsubsection{Combined Training Objective}

The total loss combines supervised learning with knowledge distillation:
\begin{equation}
    \mathcal{L}_{\text{total}} = \underbrace{\mathcal{L}_{\text{CE}}(p, y)}_{\text{hard targets}} +
    \alpha \underbrace{\mathcal{L}_{\text{time}}(\hat{t}, t)}_{\text{auxiliary}} +
    \lambda \underbrace{\mathcal{L}_{\text{KL}}^{(\tau)}(q^{(\tau)}, p^{(\tau)})}_{\text{soft targets}}
    \label{eq:total-loss}
\end{equation}
where $\alpha$ is fixed and $\lambda$ is tuned to balance supervised and distillation objectives.

\subsection{Training Pipeline}
\label{sec:method-training}

\begin{algorithm}[t]
    \caption{Knowledge Distillation Training}
    \label{alg:distillation}
    \begin{algorithmic}[1]
        \Require Dataset $\mathcal{D}$, Teacher $\mathcal{L}_\phi$, Student $\mathcal{H}_\theta$, hyperparameters $\{\lambda, \tau, w, \eta\}$
        \Ensure Trained parameters $\theta^*$
        \State Initialize $\theta$ from normal distribution
        \For{epoch $= 1$ to $E$}
        \For{$(\mathbf{r}, y) \in \mathcal{D}$}
        \State $\mathbf{z} \gets \psi(\mathbf{r})$ \Comment{Map roads to grid cells}
        \State $\mathbf{q} \gets \mathcal{L}_\phi(\mathbf{z}_{t-w:t})$ \Comment{Teacher inference (no gradient)}
        \State $q^{(\tau)} \gets \text{Renormalize}(\mathbf{q}, \mathcal{C}_t, \tau)$ \Comment{Eq.~\eqref{eq:renorm}}
        \State $\boldsymbol{\ell} \gets \mathcal{H}_\theta(\mathbf{r}, \mathcal{C}_t)$ \Comment{Student logits}
        \State $p^{(\tau)} \gets \text{Softmax}(\boldsymbol{\ell} / \tau)$
        \State $\mathcal{L} \gets \mathcal{L}_{\text{CE}}(\boldsymbol{\ell}, y) + \lambda \mathcal{L}_{\text{KL}}^{(\tau)}(q^{(\tau)}, p^{(\tau)})$
        \State $\theta \gets \theta - \eta \nabla_\theta \mathcal{L}$
        \EndFor
        \EndFor
        \State \Return $\theta^*$
    \end{algorithmic}
\end{algorithm}

The teacher parameters $\phi$ remain frozen throughout training. Per-iteration complexity is $\mathcal{O}(B \cdot t \cdot (V + k^2))$ where teacher inference dominates at $\mathcal{O}(B \cdot t \cdot V)$. Implementation details are provided in \autoref{sec:implementation}.

\subsection{Inference and Evaluation}
\label{sec:method-inference}

At inference time, we employ only the trained student $\mathcal{H}_{\theta^*}$ with beam search to generate trajectories. Given origin-destination pair $(r_o, r_d)$, the student produces complete trajectories efficiently.

We evaluate using global distribution metrics (Jensen-Shannon Divergence) and local trajectory metrics (Hausdorff, DTW, EDR) computed separately on training and test OD pairs. Full evaluation details are provided in \autoref{sec:eval-metrics}.